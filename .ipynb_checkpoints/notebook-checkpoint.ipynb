{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58a4e992-6cca-40ef-b7fe-7134727ca699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workout documents: 1\n"
     ]
    }
   ],
   "source": [
    "# 1\n",
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "import frontmatter\n",
    "\n",
    "def read_repo_data(repo_owner, repo_name):\n",
    "    \"\"\"\n",
    "    Download and parse all markdown files from a GitHub repository.\n",
    "    \n",
    "    Args:\n",
    "        repo_owner: GitHub username or organization\n",
    "        repo_name: Repository name\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing file content and metadata\n",
    "    \"\"\"\n",
    "    prefix = 'https://codeload.github.com' \n",
    "    url = f'{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/main'\n",
    "    resp = requests.get(url)\n",
    "    \n",
    "    if resp.status_code != 200:\n",
    "        raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "    repository_data = []\n",
    "    zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "    \n",
    "    for file_info in zf.infolist():\n",
    "        filename = file_info.filename\n",
    "        filename_lower = filename.lower()\n",
    "\n",
    "        if not (filename_lower.endswith('.md') \n",
    "            or filename_lower.endswith('.mdx')):\n",
    "            continue\n",
    "    \n",
    "        try:\n",
    "            with zf.open(file_info) as f_in:\n",
    "                content = f_in.read().decode('utf-8', errors='ignore')\n",
    "                post = frontmatter.loads(content)\n",
    "                data = post.to_dict()\n",
    "                data['filename'] = filename\n",
    "                repository_data.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    zf.close()\n",
    "    return repository_data\n",
    "\n",
    "workout_docs = read_repo_data('ilhamksyuriadi', 'workout-recommendation')\n",
    "# dtc_faq = read_repo_data('DataTalksClub', 'faq')\n",
    "# evidently_docs = read_repo_data('evidentlyai', 'docs')\n",
    "\n",
    "print(f\"workout documents: {len(workout_docs)}\")\n",
    "# print(f\"FAQ documents: {len(dtc_faq)}\")\n",
    "# print(f\"Evidently documents: {len(evidently_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21a99cf2-5ef6-4cbb-91d6-4d329b1f0587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 1\n",
      "Sample document keys: dict_keys(['content', 'filename'])\n",
      "Sample content length: 12897\n"
     ]
    }
   ],
   "source": [
    "# 2\n",
    "\n",
    "print(f\"Number of documents: {len(workout_docs)}\")\n",
    "print(f\"Sample document keys: {workout_docs[0].keys()}\")\n",
    "print(f\"Sample content length: {len(workout_docs[0]['content'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "854e5e3d-6f2b-4e49-87e0-f01e73d7b49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple sliding window chunks: 17\n"
     ]
    }
   ],
   "source": [
    "# 2.1 simple chunking\n",
    "def sliding_window(seq, size, step):\n",
    "    if size <= 0 or step <= 0:\n",
    "        raise ValueError(\"size and step must be positive\")\n",
    "\n",
    "    n = len(seq)\n",
    "    result = []\n",
    "    for i in range(0, n, step):\n",
    "        chunk = seq[i:i+size]\n",
    "        result.append({'start': i, 'chunk': chunk})\n",
    "        if i + size >= n:\n",
    "            break\n",
    "\n",
    "    return result\n",
    "\n",
    "workout_docs_chunks = []\n",
    "\n",
    "for doc in workout_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    chunks = sliding_window(doc_content, 1500, 750)\n",
    "    for chunk in chunks:\n",
    "        chunk.update(doc_copy)\n",
    "    workout_docs_chunks.extend(chunks)\n",
    "\n",
    "print(f\"Simple sliding window chunks: {len(workout_docs_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "744d5d85-e984-487d-8766-fb35fcdb3f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph+sliding chunks: 67\n"
     ]
    }
   ],
   "source": [
    "# 2.2 paragraph chungking + sliding windows\n",
    "import re\n",
    "def paragraph_chunking_with_sliding_window(text, max_paragraph_length=500, sliding_window_size=1000, sliding_window_step=500):\n",
    "    \"\"\"\n",
    "    Hybrid approach: Split by paragraphs, then apply sliding window to long paragraphs\n",
    "    \n",
    "    Parameters:\n",
    "    - text: The document content to chunk\n",
    "    - max_paragraph_length: If paragraph is shorter than this, keep as-is\n",
    "    - sliding_window_size: Size for sliding window (applied to long paragraphs)\n",
    "    - sliding_window_step: Step for sliding window (creates overlap)\n",
    "    \n",
    "    Returns: List of chunks with metadata\n",
    "    \"\"\"\n",
    "    # Step 1: Split into paragraphs\n",
    "    paragraphs = re.split(r\"\\n\\s*\\n\", text.strip())\n",
    "    \n",
    "    all_chunks = []\n",
    "    chunk_counter = 0\n",
    "    \n",
    "    for para_idx, paragraph in enumerate(paragraphs):\n",
    "        # Clean up the paragraph\n",
    "        paragraph = paragraph.strip()\n",
    "        if not paragraph:  # Skip empty paragraphs\n",
    "            continue\n",
    "\n",
    "        # Debug: Check if we're getting tiny paragraphs\n",
    "        if len(paragraph) < 50:  # Very short paragraph\n",
    "            # Option 1: Skip it (if it's just whitespace/formatting)\n",
    "            # Option 2: Combine with next paragraph\n",
    "            continue  # Let's skip for now\n",
    "        \n",
    "        # Step 2: Check paragraph length\n",
    "        if len(paragraph) <= max_paragraph_length:\n",
    "            # Short paragraph: keep as-is\n",
    "            chunk_info = {\n",
    "                'chunk_id': chunk_counter,\n",
    "                'paragraph_index': para_idx,\n",
    "                'chunk': paragraph,\n",
    "                'chunk_type': 'whole_paragraph',\n",
    "                'length': len(paragraph),\n",
    "                'sliding_window_info': None  # Not applicable\n",
    "            }\n",
    "            all_chunks.append(chunk_info)\n",
    "            chunk_counter += 1\n",
    "        else:\n",
    "            # Step 3: Long paragraph - apply sliding window\n",
    "            window_chunks = sliding_window(\n",
    "                paragraph, \n",
    "                sliding_window_size, \n",
    "                sliding_window_step\n",
    "            )\n",
    "            \n",
    "            # Step 4: Format each window chunk\n",
    "            for window_idx, window_chunk in enumerate(window_chunks):\n",
    "                chunk_info = {\n",
    "                    'chunk_id': chunk_counter,\n",
    "                    'paragraph_index': para_idx,\n",
    "                    'chunk': window_chunk['chunk'],\n",
    "                    'chunk_type': 'sliding_window_segment',\n",
    "                    'length': len(window_chunk['chunk']),\n",
    "                    'sliding_window_info': {\n",
    "                        'window_index': window_idx,\n",
    "                        'total_windows': len(window_chunks),\n",
    "                        'char_start': window_chunk['start'],\n",
    "                        'char_end': window_chunk['start'] + len(window_chunk['chunk']),\n",
    "                        'original_paragraph_length': len(paragraph)\n",
    "                    }\n",
    "                }\n",
    "                all_chunks.append(chunk_info)\n",
    "                chunk_counter += 1\n",
    "    \n",
    "    return all_chunks\n",
    "\n",
    "def apply_paragraph_chunking_to_documents(documents):\n",
    "    \"\"\"Apply paragraph-based chunking to all documents - FIXED\"\"\"\n",
    "    all_chunks = []\n",
    "    \n",
    "    for doc_idx, doc in enumerate(documents):\n",
    "        doc_copy = doc.copy()\n",
    "        doc_content = doc_copy.pop('content')\n",
    "        \n",
    "        # FIXED PARAMETERS:\n",
    "        chunks = paragraph_chunking_with_sliding_window(\n",
    "            doc_content,\n",
    "            max_paragraph_length=500,        # Paragraphs under 500 chars stay whole\n",
    "            sliding_window_size=1000,        # Reasonable chunk size\n",
    "            sliding_window_step=500          # 50% overlap\n",
    "        )\n",
    "        \n",
    "        # Add document metadata\n",
    "        for chunk in chunks:\n",
    "            chunk_with_metadata = doc_copy.copy()\n",
    "            chunk_with_metadata.update(chunk)\n",
    "            all_chunks.append(chunk_with_metadata)\n",
    "    \n",
    "    return all_chunks\n",
    "\n",
    "workout_docs_chunks_2 = apply_paragraph_chunking_to_documents(workout_docs)\n",
    "print(f\"Paragraph+sliding chunks: {len(workout_docs_chunks_2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "966a1c64-a59e-4578-834c-127ac827482d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section chunks (level 2): 14\n",
      "Section chunks (level 3): 28\n"
     ]
    }
   ],
   "source": [
    "# 2.3 section chunking\n",
    "def split_markdown_by_level_improved(text, level=2, include_content_before_first_header=True):\n",
    "    \"\"\"\n",
    "    Improved version that handles content before first header.\n",
    "    \n",
    "    Parameters:\n",
    "    - text: Markdown text\n",
    "    - level: Header level to split on (1 for #, 2 for ##, etc.)\n",
    "    - include_content_before_first_header: Whether to include text before first header as a section\n",
    "    \n",
    "    Returns: List of (header, content) tuples\n",
    "    \"\"\"\n",
    "    # Create the regex pattern for the specified level\n",
    "    header_pattern = r'^(#{' + str(level) + r'} )(.+)$'\n",
    "    pattern = re.compile(header_pattern, re.MULTILINE)\n",
    "    \n",
    "    # Find all header positions\n",
    "    matches = list(pattern.finditer(text))\n",
    "    \n",
    "    if not matches:\n",
    "        # No headers found at this level\n",
    "        return [('No Header', text.strip())] if text.strip() else []\n",
    "    \n",
    "    sections = []\n",
    "    \n",
    "    # Handle content before first header\n",
    "    first_match = matches[0]\n",
    "    if include_content_before_first_header and first_match.start() > 0:\n",
    "        before_content = text[:first_match.start()].strip()\n",
    "        if before_content:\n",
    "            sections.append(('Introduction', before_content))\n",
    "    \n",
    "    # Process each section\n",
    "    for i, match in enumerate(matches):\n",
    "        header_marker = match.group(1)  # e.g., \"## \"\n",
    "        header_text = match.group(2)    # e.g., \"Installation\"\n",
    "        full_header = header_marker + header_text\n",
    "        \n",
    "        # Determine the content for this section\n",
    "        if i < len(matches) - 1:\n",
    "            # Content is from after this header to before next header\n",
    "            next_match = matches[i + 1]\n",
    "            content = text[match.end():next_match.start()].strip()\n",
    "        else:\n",
    "            # Last section: content is from after header to end\n",
    "            content = text[match.end():].strip()\n",
    "        \n",
    "        sections.append((full_header, content))\n",
    "    \n",
    "    return sections\n",
    "\n",
    "def apply_section_chunking_to_documents(documents, level=2):\n",
    "    all_chunks = []\n",
    "    \n",
    "    for doc_idx, doc in enumerate(documents):\n",
    "        doc_copy = doc.copy()\n",
    "        doc_content = doc_copy.pop('content')\n",
    "        \n",
    "        sections = split_markdown_by_level_improved(doc_content, level=level)\n",
    "        \n",
    "        for section_idx, (header, content) in enumerate(sections):\n",
    "            if not content:\n",
    "                continue\n",
    "                \n",
    "            chunk_info = {\n",
    "                'chunk_id': f\"doc_{doc_idx}_sec_{section_idx}\",\n",
    "                'header': header,\n",
    "                'chunk': content,\n",
    "                'chunk_type': f'section_level_{level}',\n",
    "                'section_index': section_idx,\n",
    "                'length': len(content),\n",
    "                'has_header': header != 'No Header' and header != 'Introduction'\n",
    "            }\n",
    "            \n",
    "            chunk_with_metadata = doc_copy.copy()\n",
    "            chunk_with_metadata.update(chunk_info)\n",
    "            all_chunks.append(chunk_with_metadata)\n",
    "    \n",
    "    return all_chunks\n",
    "\n",
    "# Try multiple levels\n",
    "workout_docs_chunks_3_level2 = apply_section_chunking_to_documents(workout_docs, level=2)\n",
    "workout_docs_chunks_3_level3 = apply_section_chunking_to_documents(workout_docs, level=3)\n",
    "\n",
    "print(f\"Section chunks (level 2): {len(workout_docs_chunks_3_level2)}\")\n",
    "print(f\"Section chunks (level 3): {len(workout_docs_chunks_3_level3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7630bde-a2c2-4f33-9084-ff5d6095701b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_chunking_method(chunks, method_name):\n",
    "    \"\"\"\n",
    "    Comprehensive analysis of a chunking method\n",
    "    \"\"\"\n",
    "    if not chunks:\n",
    "        return {}\n",
    "    \n",
    "    # Extract chunk contents\n",
    "    chunk_contents = []\n",
    "    for chunk in chunks:\n",
    "        content = chunk.get('chunk') or chunk.get('section') or ''\n",
    "        chunk_contents.append(content)\n",
    "    \n",
    "    # Basic statistics\n",
    "    sizes = [len(content) for content in chunk_contents]\n",
    "    char_counts = sizes\n",
    "    \n",
    "    # Token estimation (approx: 1 token ‚âà 4 chars for English)\n",
    "    token_counts = [len(content) // 4 for content in chunk_contents]\n",
    "    \n",
    "    # Distribution analysis\n",
    "    import statistics\n",
    "    \n",
    "    metrics = {\n",
    "        'method': method_name,\n",
    "        'total_chunks': len(chunks),\n",
    "        'total_characters': sum(char_counts),\n",
    "        'avg_chars_per_chunk': statistics.mean(char_counts),\n",
    "        'median_chars_per_chunk': statistics.median(char_counts),\n",
    "        'std_chars_per_chunk': statistics.stdev(char_counts) if len(char_counts) > 1 else 0,\n",
    "        'min_chars': min(char_counts),\n",
    "        'max_chars': max(char_counts),\n",
    "        'avg_tokens_per_chunk': statistics.mean(token_counts),\n",
    "        # Distribution percentiles\n",
    "        'p25_chars': sorted(char_counts)[int(len(char_counts) * 0.25)] if char_counts else 0,\n",
    "        'p75_chars': sorted(char_counts)[int(len(char_counts) * 0.75)] if char_counts else 0,\n",
    "    }\n",
    "    \n",
    "    # Size categories (useful for visualization)\n",
    "    small = len([c for c in char_counts if c < 500])\n",
    "    medium = len([c for c in char_counts if 500 <= c < 2000])\n",
    "    large = len([c for c in char_counts if c >= 2000])\n",
    "    \n",
    "    metrics['size_distribution'] = {\n",
    "        'small_<500': small,\n",
    "        'medium_500-2000': medium,\n",
    "        'large_>=2000': large,\n",
    "        'small_percent': (small / len(char_counts) * 100) if char_counts else 0\n",
    "    }\n",
    "    \n",
    "    # Context preservation score (estimated)\n",
    "    # Count how many chunks seem \"complete\"\n",
    "    complete_chunks = 0\n",
    "    for content in chunk_contents[:100]:  # Sample first 100 for speed\n",
    "        # Simple heuristics for \"completeness\"\n",
    "        if content.strip() and len(content) > 50:\n",
    "            # Check if ends with sentence-ending punctuation\n",
    "            # FIXED LINE: Removed the extra ')'\n",
    "            if content[-1] in '.!?' or '\\n\\n' in content[-20:]:\n",
    "                complete_chunks += 1\n",
    "    \n",
    "    metrics['estimated_completeness_score'] = (\n",
    "        complete_chunks / min(100, len(chunks)) * 100 \n",
    "        if chunks else 0\n",
    "    )\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "sliding_analysis = analyze_chunking_method(workout_docs_chunks, 'sliding_window')\n",
    "paragraph_analysis = analyze_chunking_method(workout_docs_chunks_2, 'paragraph_sliding')\n",
    "section_analysis_2 = analyze_chunking_method(workout_docs_chunks_3_level2, 'section_level_2')\n",
    "section_analysis_3 = analyze_chunking_method(workout_docs_chunks_3_level3, 'section_level_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "990e5bbb-cd7a-4fd4-8b2b-d7bfb0d65dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CHUNKING METHOD COMPARISON\n",
      "======================================================================\n",
      "\n",
      "Metric                             sliding_window paragraph_slidingsection_level_2section_level_3\n",
      "--------------------------------------------------------------------------------\n",
      "total_chunks                                  17               67               14               28    \n",
      "avg_chars_per_chunk                         1465              171              901              436    \n",
      "min_chars                                    897               50              161               29    \n",
      "max_chars                                   1500              923             2864             1924    \n",
      "estimated_completeness_score                35.3%           11.9%          100.0%           14.3%  \n",
      "size_distribution.small_<500                   0               65                4               20    \n",
      "size_distribution.medium_500-2000             17                2                9                8    \n",
      "size_distribution.large_>=2000                 0                0                1                0    \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üîç KEY INSIGHTS (Look for these patterns):\n",
      "1. More chunks = better granularity for search\n",
      "2. Higher completeness score = better context preservation\n",
      "3. Balanced size distribution = good for most use cases\n",
      "4. Large max_chars might indicate poor boundary detection\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sliding_window': {'method': 'sliding_window',\n",
       "  'total_chunks': 17,\n",
       "  'total_characters': 24897,\n",
       "  'avg_chars_per_chunk': 1464.5294117647059,\n",
       "  'median_chars_per_chunk': 1500,\n",
       "  'std_chars_per_chunk': 146.2489818969088,\n",
       "  'min_chars': 897,\n",
       "  'max_chars': 1500,\n",
       "  'avg_tokens_per_chunk': 366.11764705882354,\n",
       "  'p25_chars': 1500,\n",
       "  'p75_chars': 1500,\n",
       "  'size_distribution': {'small_<500': 0,\n",
       "   'medium_500-2000': 17,\n",
       "   'large_>=2000': 0,\n",
       "   'small_percent': 0.0},\n",
       "  'estimated_completeness_score': 35.294117647058826},\n",
       " 'paragraph_sliding': {'method': 'paragraph_sliding',\n",
       "  'total_chunks': 67,\n",
       "  'total_characters': 11456,\n",
       "  'avg_chars_per_chunk': 170.98507462686567,\n",
       "  'median_chars_per_chunk': 126,\n",
       "  'std_chars_per_chunk': 162.42634624861867,\n",
       "  'min_chars': 50,\n",
       "  'max_chars': 923,\n",
       "  'avg_tokens_per_chunk': 42.298507462686565,\n",
       "  'p25_chars': 75,\n",
       "  'p75_chars': 205,\n",
       "  'size_distribution': {'small_<500': 65,\n",
       "   'medium_500-2000': 2,\n",
       "   'large_>=2000': 0,\n",
       "   'small_percent': 97.01492537313433},\n",
       "  'estimated_completeness_score': 11.940298507462686},\n",
       " 'section_level_2': {'method': 'section_level_2',\n",
       "  'total_chunks': 14,\n",
       "  'total_characters': 12614,\n",
       "  'avg_chars_per_chunk': 901,\n",
       "  'median_chars_per_chunk': 747.5,\n",
       "  'std_chars_per_chunk': 741.2287096436564,\n",
       "  'min_chars': 161,\n",
       "  'max_chars': 2864,\n",
       "  'avg_tokens_per_chunk': 224.92857142857142,\n",
       "  'p25_chars': 392,\n",
       "  'p75_chars': 1179,\n",
       "  'size_distribution': {'small_<500': 4,\n",
       "   'medium_500-2000': 9,\n",
       "   'large_>=2000': 1,\n",
       "   'small_percent': 28.57142857142857},\n",
       "  'estimated_completeness_score': 100.0},\n",
       " 'section_level_3': {'method': 'section_level_3',\n",
       "  'total_chunks': 28,\n",
       "  'total_characters': 12216,\n",
       "  'avg_chars_per_chunk': 436.2857142857143,\n",
       "  'median_chars_per_chunk': 242.5,\n",
       "  'std_chars_per_chunk': 484.4483580736048,\n",
       "  'min_chars': 29,\n",
       "  'max_chars': 1924,\n",
       "  'avg_tokens_per_chunk': 108.67857142857143,\n",
       "  'p25_chars': 121,\n",
       "  'p75_chars': 615,\n",
       "  'size_distribution': {'small_<500': 20,\n",
       "   'medium_500-2000': 8,\n",
       "   'large_>=2000': 0,\n",
       "   'small_percent': 71.42857142857143},\n",
       "  'estimated_completeness_score': 14.285714285714285}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compare_analyses(all_analyses):\n",
    "    \"\"\"\n",
    "    Compare all analysis results in a readable format\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"CHUNKING METHOD COMPARISON\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Define the metrics to compare (in order of importance)\n",
    "    key_metrics = [\n",
    "        'total_chunks',\n",
    "        'avg_chars_per_chunk', \n",
    "        'min_chars',\n",
    "        'max_chars',\n",
    "        'estimated_completeness_score',\n",
    "        'size_distribution.small_<500',\n",
    "        'size_distribution.medium_500-2000',\n",
    "        'size_distribution.large_>=2000'\n",
    "    ]\n",
    "    \n",
    "    # Print header\n",
    "    print(f\"\\n{'Metric':<35}\", end=\"\")\n",
    "    for method_name in all_analyses.keys():\n",
    "        print(f\"{method_name:<15}\", end=\"\")\n",
    "    print()\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Print each metric\n",
    "    for metric in key_metrics:\n",
    "        print(f\"{metric:<35}\", end=\"\")\n",
    "        \n",
    "        for method_name, analysis in all_analyses.items():\n",
    "            # Handle nested metrics (size_distribution.small_<500)\n",
    "            if '.' in metric:\n",
    "                parts = metric.split('.')\n",
    "                value = analysis.get(parts[0], {})\n",
    "                if isinstance(value, dict):\n",
    "                    value = value.get(parts[1], 'N/A')\n",
    "            else:\n",
    "                value = analysis.get(metric, 'N/A')\n",
    "            \n",
    "            # Format the value\n",
    "            if isinstance(value, (int, float)):\n",
    "                if metric == 'estimated_completeness_score':\n",
    "                    print(f\"{value:>13.1f}%  \", end=\"\")\n",
    "                elif metric in ['avg_chars_per_chunk']:\n",
    "                    print(f\"{value:>13.0f}    \", end=\"\")\n",
    "                else:\n",
    "                    print(f\"{value:>13}    \", end=\"\")\n",
    "            else:\n",
    "                print(f\"{str(value):>13}    \", end=\"\")\n",
    "        print()\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Add interpretation\n",
    "    print(\"\\nüîç KEY INSIGHTS (Look for these patterns):\")\n",
    "    print(\"1. More chunks = better granularity for search\")\n",
    "    print(\"2. Higher completeness score = better context preservation\")\n",
    "    print(\"3. Balanced size distribution = good for most use cases\")\n",
    "    print(\"4. Large max_chars might indicate poor boundary detection\")\n",
    "    \n",
    "    return all_analyses\n",
    "\n",
    "# Run the comparison\n",
    "\n",
    "# Compare all\n",
    "all_analyses = {\n",
    "    'sliding_window': sliding_analysis,\n",
    "    'paragraph_sliding': paragraph_analysis,\n",
    "    'section_level_2': section_analysis_2,\n",
    "    'section_level_3': section_analysis_3\n",
    "}\n",
    "\n",
    "compare_analyses(all_analyses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "656d3923-e356-49f4-b4c0-f6c670e1e2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "VISUAL COMPARISON (ASCII Charts)\n",
      "======================================================================\n",
      "\n",
      "üìä TOTAL CHUNKS (More = finer granularity):\n",
      "sliding_window               17 chunks ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "paragraph_sliding            67 chunks ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "section_level_2              14 chunks ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "section_level_3              28 chunks ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "üìè AVERAGE CHUNK SIZE (Chars):\n",
      "sliding_window             1465 chars ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "paragraph_sliding           171 chars ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "section_level_2             901 chars ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "section_level_3             436 chars ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "‚úÖ ESTIMATED COMPLETENESS SCORE (%):\n",
      "sliding_window             35.3% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "paragraph_sliding          11.9% ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "section_level_2           100.0% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "section_level_3            14.3% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "üì¶ SIZE DISTRIBUTION:\n",
      "\n",
      "sliding_window:\n",
      "  Small (<500):   0.0%\n",
      "  Medium (500-2k):‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 100.0%\n",
      "  Large (‚â•2k):     0.0%\n",
      "\n",
      "paragraph_sliding:\n",
      "  Small (<500):  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 97.0%\n",
      "  Medium (500-2k):‚ñà 3.0%\n",
      "  Large (‚â•2k):     0.0%\n",
      "\n",
      "section_level_2:\n",
      "  Small (<500):  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 28.6%\n",
      "  Medium (500-2k):‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 64.3%\n",
      "  Large (‚â•2k):    ‚ñà‚ñà‚ñà 7.1%\n",
      "\n",
      "section_level_3:\n",
      "  Small (<500):  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 71.4%\n",
      "  Medium (500-2k):‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 28.6%\n",
      "  Large (‚â•2k):     0.0%\n"
     ]
    }
   ],
   "source": [
    "def create_visual_comparison(all_analyses):\n",
    "    \"\"\"\n",
    "    Create simple bar charts for key metrics\n",
    "    \"\"\"\n",
    "    import math\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"VISUAL COMPARISON (ASCII Charts)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Chart 1: Total Chunks\n",
    "    print(\"\\nüìä TOTAL CHUNKS (More = finer granularity):\")\n",
    "    max_chunks = max([a.get('total_chunks', 0) for a in all_analyses.values()])\n",
    "    \n",
    "    for method_name, analysis in all_analyses.items():\n",
    "        chunks = analysis.get('total_chunks', 0)\n",
    "        bar_length = int((chunks / max_chunks) * 50) if max_chunks > 0 else 0\n",
    "        bar = \"‚ñà\" * bar_length\n",
    "        print(f\"{method_name:<25} {chunks:>5} chunks {bar}\")\n",
    "    \n",
    "    # Chart 2: Average Chunk Size\n",
    "    print(\"\\nüìè AVERAGE CHUNK SIZE (Chars):\")\n",
    "    max_avg = max([a.get('avg_chars_per_chunk', 0) for a in all_analyses.values()])\n",
    "    \n",
    "    for method_name, analysis in all_analyses.items():\n",
    "        avg_size = analysis.get('avg_chars_per_chunk', 0)\n",
    "        bar_length = int((avg_size / max_avg) * 50) if max_avg > 0 else 0\n",
    "        bar = \"‚ñà\" * bar_length\n",
    "        print(f\"{method_name:<25} {avg_size:>5.0f} chars {bar}\")\n",
    "    \n",
    "    # Chart 3: Completeness Score\n",
    "    print(\"\\n‚úÖ ESTIMATED COMPLETENESS SCORE (%):\")\n",
    "    \n",
    "    for method_name, analysis in all_analyses.items():\n",
    "        score = analysis.get('estimated_completeness_score', 0)\n",
    "        bar_length = int(score / 2)  # 50 chars for 100%\n",
    "        bar = \"‚ñà\" * bar_length\n",
    "        print(f\"{method_name:<25} {score:>5.1f}% {bar}\")\n",
    "    \n",
    "    # Chart 4: Size Distribution\n",
    "    print(\"\\nüì¶ SIZE DISTRIBUTION:\")\n",
    "    for method_name, analysis in all_analyses.items():\n",
    "        dist = analysis.get('size_distribution', {})\n",
    "        small = dist.get('small_<500', 0)\n",
    "        medium = dist.get('medium_500-2000', 0)\n",
    "        large = dist.get('large_>=2000', 0)\n",
    "        total = small + medium + large\n",
    "        \n",
    "        if total > 0:\n",
    "            small_pct = (small / total) * 100\n",
    "            medium_pct = (medium / total) * 100\n",
    "            large_pct = (large / total) * 100\n",
    "            \n",
    "            print(f\"\\n{method_name}:\")\n",
    "            print(f\"  Small (<500):  {'‚ñà' * int(small_pct/2)} {small_pct:.1f}%\")\n",
    "            print(f\"  Medium (500-2k):{'‚ñà' * int(medium_pct/2)} {medium_pct:.1f}%\") \n",
    "            print(f\"  Large (‚â•2k):    {'‚ñà' * int(large_pct/2)} {large_pct:.1f}%\")\n",
    "\n",
    "create_visual_comparison(all_analyses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2c708d6-ba34-452f-a3ce-e7ba3a28cdf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç MANUAL CHUNK INSPECTION - ALL METHODS\n",
      "======================================================================\n",
      "\n",
      "============================================================\n",
      "MANUAL INSPECTION: Sliding Window (size=1500, step=750)\n",
      "============================================================\n",
      "\n",
      "üìÑ Example 1/17:\n",
      "   Source: Unknown\n",
      "   Length: 1500 characters\n",
      "   Type: unknown\n",
      "   Starts with capital letter: ‚ùå\n",
      "   Ends with punctuation: ‚ùå\n",
      "   ‚ö†Ô∏è  WARNING: Starts lowercase - likely cut mid-sentence\n",
      "   ‚ö†Ô∏è  WARNING: No ending punctuation - might be incomplete\n",
      "\n",
      "   Preview (first 150 chars):\n",
      "   \"# üèãÔ∏è Workout Type Recommendation System\n",
      "\n",
      "A machine learning-based system that recommends workout types (Cardio, Strength, Yoga, or HIIT) based on user...\"\n",
      "\n",
      "   Ending (last 100 chars):\n",
      "   \"...get classes:\n",
      "- Cardio\n",
      "- Strength\n",
      "- Yoga\n",
      "- HIIT\n",
      "\n",
      "---\n",
      "\n",
      "## Dataset\n",
      "\n",
      "**Source**: [Kaggle - Gym Members E\"\n",
      "\n",
      "   --------------------------------------------------\n",
      "\n",
      "üìÑ Example 2/17:\n",
      "   Source: Unknown\n",
      "   Length: 1500 characters\n",
      "   Type: unknown\n",
      "   Starts with capital letter: ‚úÖ\n",
      "   Ends with punctuation: ‚ùå\n",
      "   ‚ö†Ô∏è  WARNING: No ending punctuation - might be incomplete\n",
      "\n",
      "   Preview (first 150 chars):\n",
      "   \"The goal is to build a machine learning model that can predict which type of workout (Cardio, Strength, Yoga, or HIIT) would be most suitable for a pe...\"\n",
      "\n",
      "   Ending (last 100 chars):\n",
      "   \"...data with consistent formats\n",
      "\n",
      "---\n",
      "\n",
      "## Project Structure\n",
      "\n",
      "```\n",
      "workout-recommendation/\n",
      "‚îÇ\n",
      "‚îú‚îÄ‚îÄ data/\n",
      "‚îÇ  \"\n",
      "\n",
      "   --------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "MANUAL INSPECTION: Paragraph + Sliding Window\n",
      "============================================================\n",
      "\n",
      "üìÑ Example 1/67:\n",
      "   Source: Unknown\n",
      "   Length: 150 characters\n",
      "   Type: whole_paragraph\n",
      "   Starts with capital letter: ‚úÖ\n",
      "   Ends with punctuation: ‚úÖ\n",
      "\n",
      "   Preview (first 150 chars):\n",
      "   \"A machine learning-based system that recommends workout types (Cardio, Strength, Yoga, or HIIT) based on user physical attributes and fitness metrics....\"\n",
      "\n",
      "   --------------------------------------------------\n",
      "\n",
      "üìÑ Example 2/67:\n",
      "   Source: Unknown\n",
      "   Length: 387 characters\n",
      "   Type: whole_paragraph\n",
      "   Starts with capital letter: ‚ùå\n",
      "   Ends with punctuation: ‚ùå\n",
      "   ‚ö†Ô∏è  WARNING: Starts lowercase - likely cut mid-sentence\n",
      "   ‚ö†Ô∏è  WARNING: No ending punctuation - might be incomplete\n",
      "\n",
      "   Preview (first 150 chars):\n",
      "   \"- [Problem Description](#problem-description)\n",
      "- [Dataset](#dataset)\n",
      "- [Project Structure](#project-structure)\n",
      "- [Installation](#installation)\n",
      "- [Runni...\"\n",
      "\n",
      "   Ending (last 100 chars):\n",
      "   \"...#deployment)\n",
      "- [Technologies Used](#technologies-used)\n",
      "- [Future Improvements](#future-improvements)\"\n",
      "\n",
      "   --------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "MANUAL INSPECTION: Section Chunking (Level 2)\n",
      "============================================================\n",
      "\n",
      "üìÑ Example 1/14:\n",
      "   Source: Unknown\n",
      "   Section: Introduction\n",
      "   Length: 191 characters\n",
      "   Type: section_level_2\n",
      "   Starts with capital letter: ‚ùå\n",
      "   Ends with punctuation: ‚úÖ\n",
      "   ‚ö†Ô∏è  WARNING: Starts lowercase - likely cut mid-sentence\n",
      "\n",
      "   Preview (first 150 chars):\n",
      "   \"# üèãÔ∏è Workout Type Recommendation System\n",
      "\n",
      "A machine learning-based system that recommends workout types (Cardio, Strength, Yoga, or HIIT) based on user...\"\n",
      "\n",
      "   --------------------------------------------------\n",
      "\n",
      "üìÑ Example 2/14:\n",
      "   Source: Unknown\n",
      "   Section: ## Table of Contents\n",
      "   Length: 392 characters\n",
      "   Type: section_level_2\n",
      "   Starts with capital letter: ‚ùå\n",
      "   Ends with punctuation: ‚ùå\n",
      "   ‚ö†Ô∏è  WARNING: Starts lowercase - likely cut mid-sentence\n",
      "   ‚ö†Ô∏è  WARNING: No ending punctuation - might be incomplete\n",
      "\n",
      "   Preview (first 150 chars):\n",
      "   \"- [Problem Description](#problem-description)\n",
      "- [Dataset](#dataset)\n",
      "- [Project Structure](#project-structure)\n",
      "- [Installation](#installation)\n",
      "- [Runni...\"\n",
      "\n",
      "   Ending (last 100 chars):\n",
      "   \"...oyment)\n",
      "- [Technologies Used](#technologies-used)\n",
      "- [Future Improvements](#future-improvements)\n",
      "\n",
      "---\"\n",
      "\n",
      "   --------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "MANUAL INSPECTION: Section Chunking (Level 3)\n",
      "============================================================\n",
      "\n",
      "üìÑ Example 1/28:\n",
      "   Source: Unknown\n",
      "   Section: Introduction\n",
      "   Length: 631 characters\n",
      "   Type: section_level_3\n",
      "   Starts with capital letter: ‚ùå\n",
      "   Ends with punctuation: ‚ùå\n",
      "   ‚ö†Ô∏è  WARNING: Starts lowercase - likely cut mid-sentence\n",
      "   ‚ö†Ô∏è  WARNING: No ending punctuation - might be incomplete\n",
      "\n",
      "   Preview (first 150 chars):\n",
      "   \"# üèãÔ∏è Workout Type Recommendation System\n",
      "\n",
      "A machine learning-based system that recommends workout types (Cardio, Strength, Yoga, or HIIT) based on user...\"\n",
      "\n",
      "   Ending (last 100 chars):\n",
      "   \"...Used](#technologies-used)\n",
      "- [Future Improvements](#future-improvements)\n",
      "\n",
      "---\n",
      "\n",
      "## Problem Description\"\n",
      "\n",
      "   --------------------------------------------------\n",
      "\n",
      "üìÑ Example 2/28:\n",
      "   Source: Unknown\n",
      "   Section: ### The Challenge\n",
      "   Length: 475 characters\n",
      "   Type: section_level_3\n",
      "   Starts with capital letter: ‚úÖ\n",
      "   Ends with punctuation: ‚ùå\n",
      "   ‚ö†Ô∏è  WARNING: No ending punctuation - might be incomplete\n",
      "\n",
      "   Preview (first 150 chars):\n",
      "   \"Recommending appropriate workout types based on user physical characteristics and fitness levels. The goal is to build a machine learning model that c...\"\n",
      "\n",
      "   Ending (last 100 chars):\n",
      "   \"... behavior (frequency, duration, calories burned)\n",
      "- Experience level (Beginner, Intermediate, Expert)\"\n",
      "\n",
      "   --------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def get_chunk_examples(chunks, method_name, num_examples=2):\n",
    "    \"\"\"Get example chunks for manual inspection\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"MANUAL INSPECTION: {method_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    if not chunks:\n",
    "        print(\"No chunks available\")\n",
    "        return\n",
    "    \n",
    "    for i in range(min(num_examples, len(chunks))):\n",
    "        chunk = chunks[i]\n",
    "        content = chunk.get('chunk', '')\n",
    "        \n",
    "        print(f\"\\nüìÑ Example {i+1}/{len(chunks)}:\")\n",
    "        print(f\"   Source: {chunk.get('title', 'Unknown')}\")\n",
    "        \n",
    "        if 'header' in chunk:\n",
    "            print(f\"   Section: {chunk.get('header', 'N/A')}\")\n",
    "        \n",
    "        print(f\"   Length: {len(content)} characters\")\n",
    "        print(f\"   Type: {chunk.get('chunk_type', 'unknown')}\")\n",
    "        \n",
    "        # Boundary analysis\n",
    "        if content:\n",
    "            starts_capital = content[0].isupper()\n",
    "            ends_punctuation = content[-1] in '.!?'\n",
    "            print(f\"   Starts with capital letter: {'‚úÖ' if starts_capital else '‚ùå'}\")\n",
    "            print(f\"   Ends with punctuation: {'‚úÖ' if ends_punctuation else '‚ùå'}\")\n",
    "            \n",
    "            # Check for mid-sentence cuts\n",
    "            if not starts_capital:\n",
    "                print(f\"   ‚ö†Ô∏è  WARNING: Starts lowercase - likely cut mid-sentence\")\n",
    "            if not ends_punctuation:\n",
    "                print(f\"   ‚ö†Ô∏è  WARNING: No ending punctuation - might be incomplete\")\n",
    "        \n",
    "        # Content preview (first and last 100 chars)\n",
    "        print(f\"\\n   Preview (first 150 chars):\")\n",
    "        print(f\"   \\\"{content[:150]}...\\\"\")\n",
    "        \n",
    "        if len(content) > 200:\n",
    "            print(f\"\\n   Ending (last 100 chars):\")\n",
    "            print(f\"   \\\"...{content[-100:]}\\\"\")\n",
    "        \n",
    "        print(f\"\\n   {'-'*50}\")\n",
    "\n",
    "# Run for ALL FOUR methods\n",
    "print(\"üîç MANUAL CHUNK INSPECTION - ALL METHODS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Sliding Window\n",
    "get_chunk_examples(workout_docs_chunks, \"Sliding Window (size=1500, step=750)\")\n",
    "\n",
    "# 2. Paragraph + Sliding Window  \n",
    "get_chunk_examples(workout_docs_chunks_2, \"Paragraph + Sliding Window\")\n",
    "\n",
    "# 3. Section Level 2\n",
    "get_chunk_examples(workout_docs_chunks_3_level2, \"Section Chunking (Level 2)\")\n",
    "\n",
    "# 4. Section Level 3\n",
    "get_chunk_examples(workout_docs_chunks_3_level3, \"Section Chunking (Level 3)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03249b9a-6726-4d08-8e92-7ddebeb893d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "COMPREHENSIVE QUALITY ASSESSMENT - ALL METHODS\n",
      "======================================================================\n",
      "\n",
      "üî¨ QUALITY ASSESSMENT: Sliding Window\n",
      "--------------------------------------------------\n",
      "Assessing 5 sample chunks...\n",
      "\n",
      "Sample 1 (1500 chars):\n",
      "  Complete: ‚ùå\n",
      "  Readable: ‚úÖ\n",
      "  Preview: # üèãÔ∏è Workout Type Recommendation System\n",
      "\n",
      "A machine learning-based system that recommends workout typ...\n",
      "\n",
      "Sample 2 (1500 chars):\n",
      "  Complete: ‚ö†Ô∏è\n",
      "  Readable: ‚úÖ\n",
      "  Preview: gitignore                               # Git ignore file\n",
      "‚îî‚îÄ‚îÄ README.md                             ...\n",
      "\n",
      "Sample 3 (1500 chars):\n",
      "  Complete: ‚ùå\n",
      "  Readable: ‚úÖ\n",
      "  Preview: th identical physical stats (same age, BMI, fitness level) can have completely different workout pre...\n",
      "\n",
      "Sample 4 (1500 chars):\n",
      "  Complete: ‚ùå\n",
      "  Readable: ‚úÖ\n",
      "  Preview: criptions:**\n",
      "\n",
      "| Field | Type | Values | Description |\n",
      "|-------|------|--------|-------------|\n",
      "| age ...\n",
      "\n",
      "Sample 5 (897 chars):\n",
      "  Complete: ‚ùå\n",
      "  Readable: ‚úÖ\n",
      "  Preview: entication\n",
      "   - Implement rate limiting\n",
      "   - Add batch prediction endpoint\n",
      "   - Create user profiles...\n",
      "\n",
      "üìà FINAL SCORES (0-1 scale):\n",
      "  Completeness   : 0.10 ‚ñà‚ñà\n",
      "  Readability    : 1.00 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Boundaries     : 0.10 ‚ñà‚ñà\n",
      "  Relevance      : 0.60 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "  Overall score: 0.45/1.0\n",
      "\n",
      "üî¨ QUALITY ASSESSMENT: Paragraph+Window\n",
      "--------------------------------------------------\n",
      "Assessing 5 sample chunks...\n",
      "\n",
      "Sample 1 (150 chars):\n",
      "  Complete: ‚úÖ\n",
      "  Readable: ‚ö†Ô∏è\n",
      "  Preview: A machine learning-based system that recommends workout types (Cardio, Strength, Yoga, or HIIT) base...\n",
      "\n",
      "Sample 2 (127 chars):\n",
      "  Complete: ‚ùå\n",
      "  Readable: ‚ùå\n",
      "  Preview: ```bash\n",
      "# Clone the repository\n",
      "git clone https://github.com/ilhamksyuriadi/workout-recommendation.gi...\n",
      "\n",
      "Sample 3 (223 chars):\n",
      "  Complete: ‚ö†Ô∏è\n",
      "  Readable: ‚ö†Ô∏è\n",
      "  Preview: Analysis shows that physical attributes have minimal variation across workout types:\n",
      "- Average BMI: ...\n",
      "\n",
      "Sample 4 (57 chars):\n",
      "  Complete: ‚ö†Ô∏è\n",
      "  Readable: ‚ùå\n",
      "  Preview: The application is deployed on Railway and accessible at:...\n",
      "\n",
      "Sample 5 (221 chars):\n",
      "  Complete: ‚ùå\n",
      "  Readable: ‚ö†Ô∏è\n",
      "  Preview: - Dataset from [Kaggle - Gym Members Exercise Dataset](https://www.kaggle.com/datasets/valakhorasani...\n",
      "\n",
      "üìà FINAL SCORES (0-1 scale):\n",
      "  Completeness   : 0.40 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Readability    : 0.30 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Boundaries     : 0.60 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Relevance      : 0.70 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "  Overall score: 0.50/1.0\n",
      "\n",
      "üî¨ QUALITY ASSESSMENT: Section Level 2\n",
      "--------------------------------------------------\n",
      "Assessing 5 sample chunks...\n",
      "\n",
      "Sample 1 (191 chars):\n",
      "  Complete: ‚ö†Ô∏è\n",
      "  Readable: ‚ö†Ô∏è\n",
      "  Preview: # üèãÔ∏è Workout Type Recommendation System\n",
      "\n",
      "A machine learning-based system that recommends workout typ...\n",
      "\n",
      "Sample 2 (718 chars):\n",
      "  Complete: ‚ùå\n",
      "  Readable: ‚úÖ\n",
      "  Preview: **Source**: [Kaggle - Gym Members Exercise Dataset](https://www.kaggle.com/datasets/valakhorasani/gy...\n",
      "\n",
      "Sample 3 (2864 chars):\n",
      "  Complete: ‚ùå\n",
      "  Readable: ‚úÖ\n",
      "  Preview: ### Models Compared\n",
      "\n",
      "| Model | Training Accuracy | Validation Accuracy | Test Accuracy |\n",
      "|-------|--...\n",
      "\n",
      "Sample 4 (544 chars):\n",
      "  Complete: ‚ùå\n",
      "  Readable: ‚úÖ\n",
      "  Preview: ### Machine Learning\n",
      "- **scikit-learn** (1.3.0) - ML algorithms and preprocessing\n",
      "- **XGBoost** (2.0...\n",
      "\n",
      "Sample 5 (226 chars):\n",
      "  Complete: ‚ùå\n",
      "  Readable: ‚ö†Ô∏è\n",
      "  Preview: - Dataset from [Kaggle - Gym Members Exercise Dataset](https://www.kaggle.com/datasets/valakhorasani...\n",
      "\n",
      "üìà FINAL SCORES (0-1 scale):\n",
      "  Completeness   : 0.10 ‚ñà‚ñà\n",
      "  Readability    : 0.80 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Boundaries     : 0.10 ‚ñà‚ñà\n",
      "  Relevance      : 0.60 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "  Overall score: 0.40/1.0\n",
      "\n",
      "üî¨ QUALITY ASSESSMENT: Section Level 3\n",
      "--------------------------------------------------\n",
      "Assessing 5 sample chunks...\n",
      "\n",
      "Sample 1 (631 chars):\n",
      "  Complete: ‚ùå\n",
      "  Readable: ‚úÖ\n",
      "  Preview: # üèãÔ∏è Workout Type Recommendation System\n",
      "\n",
      "A machine learning-based system that recommends workout typ...\n",
      "\n",
      "Sample 2 (342 chars):\n",
      "  Complete: ‚ùå\n",
      "  Readable: ‚ö†Ô∏è\n",
      "  Preview: ```bash\n",
      "# Clone the repository\n",
      "git clone https://github.com/ilhamksyuriadi/workout-recommendation.gi...\n",
      "\n",
      "Sample 3 (1924 chars):\n",
      "  Complete: ‚ùå\n",
      "  Readable: ‚úÖ\n",
      "  Preview: #### Why Is Accuracy Low?\n",
      "\n",
      "The model achieved modest accuracy (~30% validation, ~21% test) for sever...\n",
      "\n",
      "Sample 4 (193 chars):\n",
      "  Complete: ‚ùå\n",
      "  Readable: ‚ö†Ô∏è\n",
      "  Preview: - **scikit-learn** (1.3.0) - ML algorithms and preprocessing\n",
      "- **XGBoost** (2.0.0) - Gradient boosti...\n",
      "\n",
      "Sample 5 (934 chars):\n",
      "  Complete: ‚ùå\n",
      "  Readable: ‚úÖ\n",
      "  Preview: 1. **API Enhancements**\n",
      "   - Add authentication\n",
      "   - Implement rate limiting\n",
      "   - Add batch predicti...\n",
      "\n",
      "üìà FINAL SCORES (0-1 scale):\n",
      "  Completeness   : 0.00 \n",
      "  Readability    : 0.80 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Boundaries     : 0.00 \n",
      "  Relevance      : 0.50 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "  Overall score: 0.33/1.0\n"
     ]
    }
   ],
   "source": [
    "def assess_chunk_quality(chunks, method_name):\n",
    "    \"\"\"\n",
    "    Comprehensive quality assessment with scoring\n",
    "    \"\"\"\n",
    "    print(f\"\\nüî¨ QUALITY ASSESSMENT: {method_name}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    if not chunks:\n",
    "        print(\"No chunks to assess\")\n",
    "        return\n",
    "    \n",
    "    # Sample 5 chunks for assessment\n",
    "    sample_size = min(5, len(chunks))\n",
    "    sample_indices = [0, len(chunks)//4, len(chunks)//2, len(chunks)*3//4, -1]\n",
    "    sample_indices = sample_indices[:sample_size]\n",
    "    \n",
    "    scores = {\n",
    "        'completeness': 0,      # Complete sentences/thoughts\n",
    "        'readability': 0,       # Can be understood alone\n",
    "        'boundaries': 0,        # Natural start/end\n",
    "        'relevance': 0          # Contains coherent topic\n",
    "    }\n",
    "    \n",
    "    print(f\"Assessing {sample_size} sample chunks...\")\n",
    "    \n",
    "    for i, idx in enumerate(sample_indices):\n",
    "        chunk = chunks[idx]\n",
    "        content = chunk.get('chunk', '')\n",
    "        \n",
    "        print(f\"\\nSample {i+1} ({len(content)} chars):\")\n",
    "        \n",
    "        # Score completeness\n",
    "        starts_well = content and content[0].isupper()\n",
    "        ends_well = content and content[-1] in '.!?'\n",
    "        completeness = 1 if starts_well and ends_well else 0.5 if starts_well or ends_well else 0\n",
    "        scores['completeness'] += completeness\n",
    "        \n",
    "        # Score readability (simple heuristic)\n",
    "        word_count = len(content.split())\n",
    "        readability = 1 if word_count > 50 and '.' in content else 0.5 if word_count > 20 else 0\n",
    "        scores['readability'] += readability\n",
    "        \n",
    "        # Score boundaries\n",
    "        boundaries = 1 if starts_well else 0.5 if ends_well else 0\n",
    "        scores['boundaries'] += boundaries\n",
    "        \n",
    "        # Check for topic coherence (simple version)\n",
    "        unique_words = len(set(content.lower().split()[:20]))\n",
    "        relevance = 1 if unique_words < 15 else 0.5  # Fewer unique words = more focused\n",
    "        scores['relevance'] += relevance\n",
    "        \n",
    "        print(f\"  Complete: {'‚úÖ' if completeness == 1 else '‚ö†Ô∏è' if completeness == 0.5 else '‚ùå'}\")\n",
    "        print(f\"  Readable: {'‚úÖ' if readability == 1 else '‚ö†Ô∏è' if readability == 0.5 else '‚ùå'}\")\n",
    "        print(f\"  Preview: {content[:100]}...\")\n",
    "    \n",
    "    # Calculate averages\n",
    "    for key in scores:\n",
    "        scores[key] = scores[key] / sample_size\n",
    "    \n",
    "    print(f\"\\nüìà FINAL SCORES (0-1 scale):\")\n",
    "    for key, score in scores.items():\n",
    "        bar = \"‚ñà\" * int(score * 20)\n",
    "        print(f\"  {key.capitalize():<15}: {score:.2f} {bar}\")\n",
    "    \n",
    "    overall = sum(scores.values()) / len(scores)\n",
    "    print(f\"\\n  Overall score: {overall:.2f}/1.0\")\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# Assess all four methods\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPREHENSIVE QUALITY ASSESSMENT - ALL METHODS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "quality_scores = {}\n",
    "quality_scores['sliding'] = assess_chunk_quality(workout_docs_chunks, \"Sliding Window\")\n",
    "quality_scores['paragraph'] = assess_chunk_quality(workout_docs_chunks_2, \"Paragraph+Window\")\n",
    "quality_scores['section_l2'] = assess_chunk_quality(workout_docs_chunks_3_level2, \"Section Level 2\")\n",
    "quality_scores['section_l3'] = assess_chunk_quality(workout_docs_chunks_3_level3, \"Section Level 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52daa764-dc73-4515-a5b2-1859e3d25493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ COMPREHENSIVE ANALYSIS - ALL METRICS\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "COMPREHENSIVE CHUNKING ANALYSIS DASHBOARD\n",
      "====================================================================================================\n",
      "\n",
      "üìà QUANTITATIVE METRICS TABLE:\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "           Method  Total Chunks Avg Chars  Min Chars  Max Chars Auto Complete % Manual Overall Size Dist (S/M/L) Small % Comp Score Read Score Bound Score Rel Score\n",
      "   sliding_window            17      1465        897       1500           35.3%           0.00            0/17/0    0.0%       0.10       1.00        0.10      0.60\n",
      "paragraph_sliding            67       171         50        923           11.9%           0.00            65/2/0   97.0%       0.40       0.30        0.60      0.70\n",
      "  section_level_2            14       901        161       2864          100.0%           0.00             4/9/1   28.6%       0.10       0.80        0.10      0.60\n",
      "  section_level_3            28       436         29       1924           14.3%           0.00            20/8/0   71.4%       0.00       0.80        0.00      0.50\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "====================================================================================================\n",
      "VISUAL SCOREBOARD (Higher is Better)\n",
      "====================================================================================================\n",
      "\n",
      "Method              Total Chunks   Avg Size       Auto Complete  Manual Overall Readability    Boundaries     \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "sliding_window          17 N/A       1465 N/A      35.3% ‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   N/A N/A       1.00 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  0.10 ‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë\n",
      "paragraph_sliding       67 N/A        171 N/A      11.9% ‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   N/A N/A       0.30 ‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  0.60 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "section_level_2         14 N/A        901 N/A     100.0% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   N/A N/A       0.80 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë  0.10 ‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë\n",
      "section_level_3         28 N/A        436 N/A      14.3% ‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   N/A N/A       0.80 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë   N/A N/A     \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "====================================================================================================\n",
      "PERFORMANCE RADAR (1-5 Scale)\n",
      "====================================================================================================\n",
      "\n",
      "Completeness   sliding_window            1.1 ‚òÖ‚òÜ‚òÜ‚òÜ‚òÜ  paragraph_sliding         1.3 ‚òÖ‚òÜ‚òÜ‚òÜ‚òÜ  section_level_2           2.8 ‚òÖ‚òÖ‚òÜ‚òÜ‚òÜ  section_level_3           0.4 ‚òÜ‚òÜ‚òÜ‚òÜ‚òÜ  \n",
      "\n",
      "Readability    sliding_window            5.0 ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ  paragraph_sliding         1.5 ‚òÖ‚òÜ‚òÜ‚òÜ‚òÜ  section_level_2           4.0 ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ  section_level_3           4.0 ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ  \n",
      "\n",
      "Size Balance   sliding_window            2.7 ‚òÖ‚òÖ‚òÜ‚òÜ‚òÜ  paragraph_sliding         0.9 ‚òÜ‚òÜ‚òÜ‚òÜ‚òÜ  section_level_2           4.5 ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ  section_level_3           2.2 ‚òÖ‚òÖ‚òÜ‚òÜ‚òÜ  \n",
      "\n",
      "Usefulness     sliding_window            3.0 ‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ  paragraph_sliding         3.5 ‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ  section_level_2           3.0 ‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ  section_level_3           2.5 ‚òÖ‚òÖ‚òÜ‚òÜ‚òÜ  \n",
      "\n",
      "Boundary Qualitysliding_window            0.5 ‚òÜ‚òÜ‚òÜ‚òÜ‚òÜ  paragraph_sliding         3.0 ‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ  section_level_2           0.5 ‚òÜ‚òÜ‚òÜ‚òÜ‚òÜ  section_level_3           0.0 ‚òÜ‚òÜ‚òÜ‚òÜ‚òÜ  \n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "OVERALL SCORES (Average of 5 criteria):\n",
      "sliding_window           : 2.46/5.0 \n",
      "paragraph_sliding        : 2.03/5.0 \n",
      "section_level_2          : 2.95/5.0 üèÜ\n",
      "section_level_3          : 1.81/5.0 \n",
      "\n",
      "====================================================================================================\n",
      "SAMPLE CHUNKS COMPARISON (First 2 chunks)\n",
      "====================================================================================================\n",
      "\n",
      "================================================== SAMPLE #1 ==================================================\n",
      "\n",
      "üìÑ SLIDING_WINDOW:\n",
      "   Size: 1500  chars  |  Source: Unknown                       \n",
      "   Preview: # üèãÔ∏è Workout Type Recommendation System  A machine learning-based system that\n",
      "                recommends workout types (Cardio, Strength, Yoga, or HIIT) based on user\n",
      "                physical attributes and fitness metrics.  ## Tabl...\n",
      "   Quality: ‚ùå Start | ‚ùå End\n",
      "\n",
      "üìÑ PARAGRAPH_SLIDING:\n",
      "   Size: 150   chars  |  Source: Unknown                       \n",
      "   Preview: A machine learning-based system that recommends workout types (Cardio, Strength,\n",
      "                Yoga, or HIIT) based on user physical attributes and fitness metrics.\n",
      "   Quality: ‚úÖ Start | ‚úÖ End\n",
      "\n",
      "üìÑ SECTION_LEVEL_2:\n",
      "   Size: 191   chars  |  Source: Unknown                       \n",
      "   Header: Introduction\n",
      "   Preview: # üèãÔ∏è Workout Type Recommendation System  A machine learning-based system that\n",
      "                recommends workout types (Cardio, Strength, Yoga, or HIIT) based on user\n",
      "                physical attributes and fitness metrics.\n",
      "   Quality: ‚ùå Start | ‚úÖ End\n",
      "\n",
      "üìÑ SECTION_LEVEL_3:\n",
      "   Size: 631   chars  |  Source: Unknown                       \n",
      "   Header: Introduction\n",
      "   Preview: # üèãÔ∏è Workout Type Recommendation System  A machine learning-based system that\n",
      "                recommends workout types (Cardio, Strength, Yoga, or HIIT) based on user\n",
      "                physical attributes and fitness metrics.  ## Tabl...\n",
      "   Quality: ‚ùå Start | ‚ùå End\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "================================================== SAMPLE #2 ==================================================\n",
      "\n",
      "üìÑ SLIDING_WINDOW:\n",
      "   Size: 1500  chars  |  Source: Unknown                       \n",
      "   Preview: The goal is to build a machine learning model that can predict which type of\n",
      "                workout (Cardio, Strength, Yoga, or HIIT) would be most suitable for a person\n",
      "                based on their:  - Physical attributes (age, ...\n",
      "   Quality: ‚úÖ Start | ‚ùå End\n",
      "\n",
      "üìÑ PARAGRAPH_SLIDING:\n",
      "   Size: 387   chars  |  Source: Unknown                       \n",
      "   Preview: - [Problem Description](#problem-description) - [Dataset](#dataset) - [Project\n",
      "                Structure](#project-structure) - [Installation](#installation) - [Running the\n",
      "                Project](#running-the-project) - [Model Per...\n",
      "   Quality: ‚ùå Start | ‚ùå End\n",
      "\n",
      "üìÑ SECTION_LEVEL_2:\n",
      "   Size: 392   chars  |  Source: Unknown                       \n",
      "   Header: ## Table of Contents\n",
      "   Preview: - [Problem Description](#problem-description) - [Dataset](#dataset) - [Project\n",
      "                Structure](#project-structure) - [Installation](#installation) - [Running the\n",
      "                Project](#running-the-project) - [Model Per...\n",
      "   Quality: ‚ùå Start | ‚ùå End\n",
      "\n",
      "üìÑ SECTION_LEVEL_3:\n",
      "   Size: 475   chars  |  Source: Unknown                       \n",
      "   Header: ### The Challenge\n",
      "   Preview: Recommending appropriate workout types based on user physical characteristics\n",
      "                and fitness levels. The goal is to build a machine learning model that can\n",
      "                predict which type of workout (Cardio, Strength...\n",
      "   Quality: ‚úÖ Start | ‚ùå End\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "====================================================================================================\n",
      "üéØ EXECUTIVE SUMMARY\n",
      "====================================================================================================\n",
      "\n",
      "üèÜ RANKING (Weighted Score):\n",
      "ü•á section_level_2     : 52.3/100\n",
      "ü•à sliding_window      : 40.6/100\n",
      "ü•â section_level_3     : 18.0/100\n",
      "4. paragraph_sliding   : 7.2/100\n",
      "\n",
      "üìã RECOMMENDATION:\n",
      "1. Use 'section_level_2' for best overall performance\n",
      "2. Consider 'sliding_window' as alternative\n",
      "3. Avoid 'paragraph_sliding' (lowest score)\n",
      "\n",
      "====================================================================================================\n",
      "ANALYSIS COMPLETE - All metrics displayed above\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from textwrap import wrap\n",
    "\n",
    "def create_comprehensive_comparison(all_chunk_sets, all_analyses, quality_scores):\n",
    "    \"\"\"\n",
    "    Create a complete dashboard showing ALL metrics\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"COMPREHENSIVE CHUNKING ANALYSIS DASHBOARD\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    # Prepare data\n",
    "    methods = list(all_analyses.keys())\n",
    "    \n",
    "    # Create summary table\n",
    "    summary_data = []\n",
    "    \n",
    "    for method in methods:\n",
    "        analysis = all_analyses.get(method, {})\n",
    "        quality = quality_scores.get(method.replace('sliding_window', 'sliding')\n",
    "                                     .replace('paragraph_sliding', 'paragraph')\n",
    "                                     .replace('section_level_2', 'section_l2')\n",
    "                                     .replace('section_level_3', 'section_l3'), {})\n",
    "        \n",
    "        # Extract metrics\n",
    "        row = {\n",
    "            'Method': method,\n",
    "            'Total Chunks': analysis.get('total_chunks', 0),\n",
    "            'Avg Chars': f\"{analysis.get('avg_chars_per_chunk', 0):.0f}\",\n",
    "            'Min Chars': analysis.get('min_chars', 0),\n",
    "            'Max Chars': analysis.get('max_chars', 0),\n",
    "            'Auto Complete %': f\"{analysis.get('estimated_completeness_score', 0):.1f}%\",\n",
    "            'Manual Overall': f\"{quality.get('overall', 0):.2f}\" if quality else \"N/A\",\n",
    "            'Size Dist (S/M/L)': f\"{analysis.get('size_distribution', {}).get('small_<500', 0)}/{analysis.get('size_distribution', {}).get('medium_500-2000', 0)}/{analysis.get('size_distribution', {}).get('large_>=2000', 0)}\",\n",
    "            'Small %': f\"{analysis.get('size_distribution', {}).get('small_percent', 0):.1f}%\",\n",
    "        }\n",
    "        \n",
    "        # Add quality sub-scores if available\n",
    "        if quality:\n",
    "            row.update({\n",
    "                'Comp Score': f\"{quality.get('completeness', 0):.2f}\",\n",
    "                'Read Score': f\"{quality.get('readability', 0):.2f}\",\n",
    "                'Bound Score': f\"{quality.get('boundaries', 0):.2f}\",\n",
    "                'Rel Score': f\"{quality.get('relevance', 0):.2f}\",\n",
    "            })\n",
    "        \n",
    "        summary_data.append(row)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    # Display as table\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', 120)\n",
    "    \n",
    "    print(\"\\nüìà QUANTITATIVE METRICS TABLE:\")\n",
    "    print(\"-\" * 120)\n",
    "    print(df.to_string(index=False))\n",
    "    print(\"-\" * 120)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_visual_scoreboard(all_analyses, quality_scores):\n",
    "    \"\"\"\n",
    "    Create a visual scoreboard with color coding\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"VISUAL SCOREBOARD (Higher is Better)\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    methods = list(all_analyses.keys())\n",
    "    \n",
    "    # Define metrics to display\n",
    "    metrics = [\n",
    "        ('Total Chunks', 'total_chunks', 'numeric', False),  # Not always \"higher is better\"\n",
    "        ('Avg Size', 'avg_chars_per_chunk', 'numeric', True),  # Optimal range is better\n",
    "        ('Auto Complete', 'estimated_completeness_score', 'percent', True),\n",
    "        ('Manual Overall', 'overall', 'score', True),\n",
    "        ('Readability', 'readability', 'score', True),\n",
    "        ('Boundaries', 'boundaries', 'score', True),\n",
    "    ]\n",
    "    \n",
    "    # Get max values for scaling\n",
    "    max_values = {}\n",
    "    for metric_name, metric_key, metric_type, higher_is_better in metrics:\n",
    "        values = []\n",
    "        for method in methods:\n",
    "            if metric_type == 'score':\n",
    "                # Get from quality_scores\n",
    "                method_key = method.replace('sliding_window', 'sliding')\\\n",
    "                                 .replace('paragraph_sliding', 'paragraph')\\\n",
    "                                 .replace('section_level_2', 'section_l2')\\\n",
    "                                 .replace('section_level_3', 'section_l3')\n",
    "                val = quality_scores.get(method_key, {}).get(metric_key, 0)\n",
    "            else:\n",
    "                # Get from all_analyses\n",
    "                val = all_analyses.get(method, {}).get(metric_key, 0)\n",
    "            if val:\n",
    "                values.append(float(str(val).replace('%', '')))\n",
    "        max_values[metric_key] = max(values) if values else 1\n",
    "    \n",
    "    # Print header\n",
    "    print(f\"\\n{'Method':<20}\", end=\"\")\n",
    "    for metric_name, _, _, _ in metrics:\n",
    "        print(f\"{metric_name:<15}\", end=\"\")\n",
    "    print(\"\\n\" + \"-\" * (20 + 15 * len(metrics)))\n",
    "    \n",
    "    # Print each method\n",
    "    for method in methods:\n",
    "        print(f\"{method:<20}\", end=\"\")\n",
    "        \n",
    "        for metric_name, metric_key, metric_type, higher_is_better in metrics:\n",
    "            if metric_type == 'score':\n",
    "                method_key = method.replace('sliding_window', 'sliding')\\\n",
    "                                 .replace('paragraph_sliding', 'paragraph')\\\n",
    "                                 .replace('section_level_2', 'section_l2')\\\n",
    "                                 .replace('section_level_3', 'section_l3')\n",
    "                val = quality_scores.get(method_key, {}).get(metric_key, 0)\n",
    "                display_val = f\"{val:.2f}\" if val else \"N/A\"\n",
    "            else:\n",
    "                val = all_analyses.get(method, {}).get(metric_key, 0)\n",
    "                if metric_type == 'percent':\n",
    "                    display_val = f\"{val:.1f}%\" if val else \"N/A\"\n",
    "                else:\n",
    "                    display_val = f\"{val:.0f}\" if val else \"N/A\"\n",
    "            \n",
    "            # Add visual indicator\n",
    "            if val and metric_type in ['percent', 'score']:\n",
    "                normalized = float(str(val).replace('%', '')) / max_values[metric_key]\n",
    "                bars = int(normalized * 10)\n",
    "                visual = \"‚ñà\" * bars + \"‚ñë\" * (10 - bars)\n",
    "                print(f\"{display_val:>6} {visual:<8}\", end=\"\")\n",
    "            else:\n",
    "                print(f\"{display_val:>6} {'N/A':<8}\", end=\"\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    print(\"-\" * (20 + 15 * len(metrics)))\n",
    "\n",
    "def create_radar_chart_data(all_analyses, quality_scores):\n",
    "    \"\"\"\n",
    "    Prepare data for a \"text-based radar chart\"\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"PERFORMANCE RADAR (1-5 Scale)\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    methods = list(all_analyses.keys())\n",
    "    criteria = ['Completeness', 'Readability', 'Size Balance', 'Usefulness', 'Boundary Quality']\n",
    "    \n",
    "    # Score each method on each criterion (1-5)\n",
    "    scores = {}\n",
    "    \n",
    "    for method in methods:\n",
    "        analysis = all_analyses.get(method, {})\n",
    "        method_key = method.replace('sliding_window', 'sliding')\\\n",
    "                         .replace('paragraph_sliding', 'paragraph')\\\n",
    "                         .replace('section_level_2', 'section_l2')\\\n",
    "                         .replace('section_level_3', 'section_l3')\n",
    "        quality = quality_scores.get(method_key, {})\n",
    "        \n",
    "        method_scores = []\n",
    "        \n",
    "        # 1. Completeness (auto + manual combined)\n",
    "        auto_comp = analysis.get('estimated_completeness_score', 0) / 100\n",
    "        manual_comp = quality.get('completeness', 0)\n",
    "        completeness = (auto_comp + manual_comp) / 2 * 5\n",
    "        method_scores.append(min(5, completeness))\n",
    "        \n",
    "        # 2. Readability\n",
    "        readability = quality.get('readability', 0) * 5\n",
    "        method_scores.append(min(5, readability))\n",
    "        \n",
    "        # 3. Size Balance (ideal: not too small, not too large)\n",
    "        avg_size = analysis.get('avg_chars_per_chunk', 0)\n",
    "        # Score: 5 if ~1000 chars, lower if far from ideal\n",
    "        size_score = 5 * (1 - min(abs(avg_size - 1000) / 1000, 1))\n",
    "        method_scores.append(min(5, size_score))\n",
    "        \n",
    "        # 4. Usefulness (estimated from relevance)\n",
    "        usefulness = quality.get('relevance', 0) * 5\n",
    "        method_scores.append(min(5, usefulness))\n",
    "        \n",
    "        # 5. Boundary Quality\n",
    "        boundaries = quality.get('boundaries', 0) * 5\n",
    "        method_scores.append(min(5, boundaries))\n",
    "        \n",
    "        scores[method] = method_scores\n",
    "    \n",
    "    # Print radar chart\n",
    "    max_score = 5\n",
    "    \n",
    "    for criterion_idx, criterion in enumerate(criteria):\n",
    "        print(f\"\\n{criterion:<15}\", end=\"\")\n",
    "        for method in methods:\n",
    "            score = scores[method][criterion_idx]\n",
    "            bars = int(score)\n",
    "            visual = \"‚òÖ\" * bars + \"‚òÜ\" * (5 - bars)\n",
    "            print(f\"{method:<25} {score:3.1f} {visual}\", end=\"  \")\n",
    "        print()\n",
    "    \n",
    "    # Calculate overall scores\n",
    "    print(\"\\n\" + \"-\" * 100)\n",
    "    print(\"OVERALL SCORES (Average of 5 criteria):\")\n",
    "    for method in methods:\n",
    "        overall = sum(scores[method]) / len(scores[method])\n",
    "        print(f\"{method:<25}: {overall:.2f}/5.0 {'üèÜ' if overall == max([sum(scores[m])/len(scores[m]) for m in methods]) else ''}\")\n",
    "\n",
    "def show_chunk_samples_side_by_side(all_chunk_sets, num_samples=2):\n",
    "    \"\"\"\n",
    "    Show actual chunk samples from each method side-by-side\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(f\"SAMPLE CHUNKS COMPARISON (First {num_samples} chunks)\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    methods = list(all_chunk_sets.keys())\n",
    "    \n",
    "    for sample_idx in range(num_samples):\n",
    "        print(f\"\\n{'='*50} SAMPLE #{sample_idx+1} {'='*50}\")\n",
    "        \n",
    "        for method in methods:\n",
    "            chunks = all_chunk_sets[method]\n",
    "            if sample_idx < len(chunks):\n",
    "                chunk = chunks[sample_idx]\n",
    "                content = chunk.get('chunk', 'No content')\n",
    "                \n",
    "                print(f\"\\nüìÑ {method.upper()}:\")\n",
    "                print(f\"   Size: {len(content):<5} chars  |  Source: {chunk.get('title', 'Unknown'):<30}\")\n",
    "                if 'header' in chunk:\n",
    "                    print(f\"   Header: {chunk.get('header', '')}\")\n",
    "                \n",
    "                # Show preview (first and last 100 chars)\n",
    "                preview = content[:200] + \"...\" if len(content) > 200 else content\n",
    "                wrapped = \"\\n                \".join(wrap(preview, width=80))\n",
    "                print(f\"   Preview: {wrapped}\")\n",
    "                \n",
    "                # Quality indicators\n",
    "                if content:\n",
    "                    starts_ok = content[0].isupper()\n",
    "                    ends_ok = content[-1] in '.!?'\n",
    "                    print(f\"   Quality: {'‚úÖ' if starts_ok else '‚ùå'} Start | {'‚úÖ' if ends_ok else '‚ùå'} End\")\n",
    "            else:\n",
    "                print(f\"\\nüìÑ {method.upper()}: No chunk at index {sample_idx}\")\n",
    "        \n",
    "        print(\"\\n\" + \"-\" * 100)\n",
    "\n",
    "# ===== RUN EVERYTHING =====\n",
    "\n",
    "print(\"üöÄ COMPREHENSIVE ANALYSIS - ALL METRICS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Prepare your data\n",
    "all_chunk_sets = {\n",
    "    'sliding_window': workout_docs_chunks,\n",
    "    'paragraph_sliding': workout_docs_chunks_2,\n",
    "    'section_level_2': workout_docs_chunks_3_level2,\n",
    "    'section_level_3': workout_docs_chunks_3_level3\n",
    "}\n",
    "\n",
    "all_analyses = {\n",
    "    'sliding_window': sliding_analysis,\n",
    "    'paragraph_sliding': paragraph_analysis,\n",
    "    'section_level_2': section_analysis_2,\n",
    "    'section_level_3': section_analysis_3\n",
    "}\n",
    "\n",
    "# You need to run analysis for section level 3\n",
    "section_chunk_analysis_3 = analyze_chunking_method(workout_docs_chunks_3_level3, 'section_level_3')\n",
    "\n",
    "# Run all visualizations\n",
    "df = create_comprehensive_comparison(all_chunk_sets, all_analyses, quality_scores)\n",
    "create_visual_scoreboard(all_analyses, quality_scores)\n",
    "create_radar_chart_data(all_analyses, quality_scores)\n",
    "show_chunk_samples_side_by_side(all_chunk_sets, num_samples=2)\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"üéØ EXECUTIVE SUMMARY\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Calculate winner based on multiple criteria\n",
    "def determine_winner(all_analyses, quality_scores):\n",
    "    methods = list(all_analyses.keys())\n",
    "    weighted_scores = []\n",
    "    \n",
    "    for method in methods:\n",
    "        analysis = all_analyses[method]\n",
    "        method_key = method.replace('sliding_window', 'sliding')\\\n",
    "                         .replace('paragraph_sliding', 'paragraph')\\\n",
    "                         .replace('section_level_2', 'section_l2')\\\n",
    "                         .replace('section_level_3', 'section_l3')\n",
    "        quality = quality_scores.get(method_key, {})\n",
    "        \n",
    "        # Weighted scoring (adjust weights based on your use case)\n",
    "        score = (\n",
    "            analysis.get('estimated_completeness_score', 0) * 0.3 +          # Auto complete\n",
    "            quality.get('overall', 0) * 100 * 0.4 +                          # Manual overall\n",
    "            (1 - analysis.get('size_distribution', {}).get('small_percent', 0)/100) * 100 * 0.2 +  # Not too small\n",
    "            quality.get('readability', 0) * 100 * 0.1                        # Readability\n",
    "        )\n",
    "        \n",
    "        weighted_scores.append((method, score))\n",
    "    \n",
    "    return sorted(weighted_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "winners = determine_winner(all_analyses, quality_scores)\n",
    "\n",
    "print(\"\\nüèÜ RANKING (Weighted Score):\")\n",
    "for i, (method, score) in enumerate(winners, 1):\n",
    "    medal = \"ü•á\" if i == 1 else \"ü•à\" if i == 2 else \"ü•â\" if i == 3 else f\"{i}.\"\n",
    "    print(f\"{medal} {method:<20}: {score:.1f}/100\")\n",
    "\n",
    "print(\"\\nüìã RECOMMENDATION:\")\n",
    "print(f\"1. Use '{winners[0][0]}' for best overall performance\")\n",
    "print(f\"2. Consider '{winners[1][0]}' as alternative\")\n",
    "print(f\"3. Avoid '{winners[-1][0]}' (lowest score)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"ANALYSIS COMPLETE - All metrics displayed above\")\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d08ce90-5555-434e-87d6-4ab6ef6d0e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available fields in chunks: ['filename', 'chunk_id', 'header', 'chunk', 'chunk_type', 'section_index', 'length', 'has_header']\n",
      "Creating text search index with fields: ['chunk', 'header', 'filename']\n",
      "‚úÖ Text search index created with 14 chunks\n"
     ]
    }
   ],
   "source": [
    "# 3\n",
    "# choosen chunking method: section level 2\n",
    "from minsearch import Index\n",
    "workout_chunks = workout_docs_chunks_3_level2  # Section chunking was best\n",
    "\n",
    "def setup_text_search(chunks):\n",
    "    \"\"\"\n",
    "    Setup text search index for your chunks - UPDATED VERSION\n",
    "    \"\"\"\n",
    "    # Check what fields actually exist\n",
    "    sample_chunk = chunks[0]\n",
    "    available_fields = list(sample_chunk.keys())\n",
    "    print(f\"Available fields in chunks: {available_fields}\")\n",
    "    \n",
    "    # Choose fields that contain searchable text\n",
    "    # Your chunks have: ['filename', 'chunk_id', 'header', 'chunk', 'chunk_type', 'section_index', 'length', 'has_header']\n",
    "    text_fields = [\"chunk\", \"header\", \"filename\"]  # These have searchable text\n",
    "    \n",
    "    print(f\"Creating text search index with fields: {text_fields}\")\n",
    "    \n",
    "    index = Index(\n",
    "        text_fields=text_fields,\n",
    "        keyword_fields=[]\n",
    "    )\n",
    "    \n",
    "    index.fit(chunks)\n",
    "    print(f\"‚úÖ Text search index created with {len(chunks)} chunks\")\n",
    "    return index\n",
    "\n",
    "# Recreate text index with correct fields\n",
    "text_index = setup_text_search(workout_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12f94484-752d-433a-81af-b7c81f9ce37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: multi-qa-distilbert-cos-v1\n",
      "Creating embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a728ef6d5e0f4b51951bd74dfed4372b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Vector search index created with 14 embeddings\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from minsearch import VectorSearch\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def setup_vector_search(chunks, model_name='multi-qa-distilbert-cos-v1'):\n",
    "    \"\"\"\n",
    "    Setup vector search with embeddings - UPDATED VERSION\n",
    "    \"\"\"\n",
    "    # Load embedding model\n",
    "    print(f\"Loading embedding model: {model_name}\")\n",
    "    embedding_model = SentenceTransformer(model_name)\n",
    "    \n",
    "    # Create embeddings for all chunks\n",
    "    print(\"Creating embeddings...\")\n",
    "    embeddings = []\n",
    "    \n",
    "    for chunk in tqdm(chunks):\n",
    "        # Combine relevant fields for embedding\n",
    "        # Your chunks don't have 'title', use 'header' instead\n",
    "        text = chunk.get('chunk', '')\n",
    "        \n",
    "        # Add header for context (if exists)\n",
    "        if 'header' in chunk:\n",
    "            text = chunk['header'] + \" \" + text\n",
    "        \n",
    "        # Optionally add filename for more context\n",
    "        if 'filename' in chunk:\n",
    "            # Extract just the filename, not full path\n",
    "            filename = chunk['filename'].split('/')[-1]\n",
    "            text = filename + \" \" + text\n",
    "            \n",
    "        v = embedding_model.encode(text)\n",
    "        embeddings.append(v)\n",
    "    \n",
    "    embeddings = np.array(embeddings)\n",
    "    \n",
    "    # Create vector search index\n",
    "    vector_index = VectorSearch()\n",
    "    vector_index.fit(embeddings, chunks)\n",
    "    \n",
    "    print(f\"‚úÖ Vector search index created with {len(embeddings)} embeddings\")\n",
    "    return vector_index, embedding_model\n",
    "\n",
    "# Create vector search index\n",
    "vector_index, embedding_model = setup_vector_search(workout_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f3d776a-5226-4269-95fa-843a51fe1102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RE-TESTING WITH UPDATED BALANCED HYBRID SEARCH\n",
      "================================================================================\n",
      "\n",
      "üìä TEST 1/3\n",
      "\n",
      "üîç QUERY: 'installation'\n",
      "============================================================\n",
      "\n",
      "üìÑ TEXT SEARCH:\n",
      "Found 2 results:\n",
      "\n",
      "1. üìÑ README.md\n",
      "   üìç ## Installation\n",
      "   üîç ### Prerequisites\n",
      "\n",
      "- Python 3.9+\n",
      "- pip\n",
      "- Docker (optional, for containerization)\n",
      "\n",
      "### Option 1: Loca...\n",
      "\n",
      "2. üìÑ README.md\n",
      "   üìç ## Table of Contents\n",
      "   üîç - [Problem Description](#problem-description)\n",
      "- [Dataset](#dataset)\n",
      "- [Project Structure](#project-s...\n",
      "\n",
      "üß† VECTOR SEARCH:\n",
      "Found 3 results:\n",
      "\n",
      "1. üìÑ README.md\n",
      "   üìç ## Installation\n",
      "   üîç ### Prerequisites\n",
      "\n",
      "- Python 3.9+\n",
      "- pip\n",
      "- Docker (optional, for containerization)\n",
      "\n",
      "### Option 1: Loca...\n",
      "\n",
      "2. üìÑ README.md\n",
      "   üìç ## Deployment\n",
      "   üîç ### Local Deployment with Docker\n",
      "\n",
      "```bash\n",
      "# Build the Docker image\n",
      "docker build -t workout-recommend...\n",
      "\n",
      "3. üìÑ README.md\n",
      "   üìç ## Table of Contents\n",
      "   üîç - [Problem Description](#problem-description)\n",
      "- [Dataset](#dataset)\n",
      "- [Project Structure](#project-s...\n",
      "\n",
      "ü§ù HYBRID SEARCH (BALANCED):\n",
      "Found 3 results:\n",
      "\n",
      "1. üìÑ README.md [text]\n",
      "   üìç ## Installation\n",
      "   üîç ### Prerequisites\n",
      "\n",
      "- Python 3.9+\n",
      "- pip\n",
      "- Docker (optional, for containerization)\n",
      "\n",
      "### Option 1: Loca...\n",
      "\n",
      "2. üìÑ README.md [text]\n",
      "   üìç ## Table of Contents\n",
      "   üîç - [Problem Description](#problem-description)\n",
      "- [Dataset](#dataset)\n",
      "- [Project Structure](#project-s...\n",
      "\n",
      "3. üìÑ README.md [vector]\n",
      "   üìç ## Deployment\n",
      "   üîç ### Local Deployment with Docker\n",
      "\n",
      "```bash\n",
      "# Build the Docker image\n",
      "docker build -t workout-recommend...\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "üìä TEST 2/3\n",
      "\n",
      "üîç QUERY: 'how to setup'\n",
      "============================================================\n",
      "\n",
      "üìÑ TEXT SEARCH:\n",
      "Found 3 results:\n",
      "\n",
      "1. üìÑ README.md\n",
      "   üìç ## Installation\n",
      "   üîç ### Prerequisites\n",
      "\n",
      "- Python 3.9+\n",
      "- pip\n",
      "- Docker (optional, for containerization)\n",
      "\n",
      "### Option 1: Loca...\n",
      "\n",
      "2. üìÑ README.md\n",
      "   üìç ## Running the Project\n",
      "   üîç ### 1. Data Exploration (Optional)\n",
      "\n",
      "```bash\n",
      "# Open Jupyter notebook\n",
      "jupyter notebook notebook.ipynb\n",
      "...\n",
      "\n",
      "3. üìÑ README.md\n",
      "   üìç ## Model Performance\n",
      "   üîç ### Models Compared\n",
      "\n",
      "| Model | Training Accuracy | Validation Accuracy | Test Accuracy |\n",
      "|-------|--...\n",
      "\n",
      "üß† VECTOR SEARCH:\n",
      "Found 3 results:\n",
      "\n",
      "1. üìÑ README.md\n",
      "   üìç ## Deployment\n",
      "   üîç ### Local Deployment with Docker\n",
      "\n",
      "```bash\n",
      "# Build the Docker image\n",
      "docker build -t workout-recommend...\n",
      "\n",
      "2. üìÑ README.md\n",
      "   üìç ## Table of Contents\n",
      "   üîç - [Problem Description](#problem-description)\n",
      "- [Dataset](#dataset)\n",
      "- [Project Structure](#project-s...\n",
      "\n",
      "3. üìÑ README.md\n",
      "   üìç ## Installation\n",
      "   üîç ### Prerequisites\n",
      "\n",
      "- Python 3.9+\n",
      "- pip\n",
      "- Docker (optional, for containerization)\n",
      "\n",
      "### Option 1: Loca...\n",
      "\n",
      "ü§ù HYBRID SEARCH (BALANCED):\n",
      "Found 3 results:\n",
      "\n",
      "1. üìÑ README.md [text]\n",
      "   üìç ## Installation\n",
      "   üîç ### Prerequisites\n",
      "\n",
      "- Python 3.9+\n",
      "- pip\n",
      "- Docker (optional, for containerization)\n",
      "\n",
      "### Option 1: Loca...\n",
      "\n",
      "2. üìÑ README.md [vector]\n",
      "   üìç ## Deployment\n",
      "   üîç ### Local Deployment with Docker\n",
      "\n",
      "```bash\n",
      "# Build the Docker image\n",
      "docker build -t workout-recommend...\n",
      "\n",
      "3. üìÑ README.md [text]\n",
      "   üìç ## Running the Project\n",
      "   üîç ### 1. Data Exploration (Optional)\n",
      "\n",
      "```bash\n",
      "# Open Jupyter notebook\n",
      "jupyter notebook notebook.ipynb\n",
      "...\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "üìä TEST 3/3\n",
      "\n",
      "üîç QUERY: 'machine learning'\n",
      "============================================================\n",
      "\n",
      "üìÑ TEXT SEARCH:\n",
      "Found 3 results:\n",
      "\n",
      "1. üìÑ README.md\n",
      "   üìç Introduction\n",
      "   üîç # üèãÔ∏è Workout Type Recommendation System\n",
      "\n",
      "A machine learning-based system that recommends workout typ...\n",
      "\n",
      "2. üìÑ README.md\n",
      "   üìç ## Technologies Used\n",
      "   üîç ### Machine Learning\n",
      "- **scikit-learn** (1.3.0) - ML algorithms and preprocessing\n",
      "- **XGBoost** (2.0...\n",
      "\n",
      "3. üìÑ README.md\n",
      "   üìç ## Problem Description\n",
      "   üîç ### The Challenge\n",
      "\n",
      "Recommending appropriate workout types based on user physical characteristics and...\n",
      "\n",
      "üß† VECTOR SEARCH:\n",
      "Found 3 results:\n",
      "\n",
      "1. üìÑ README.md\n",
      "   üìç ## Technologies Used\n",
      "   üîç ### Machine Learning\n",
      "- **scikit-learn** (1.3.0) - ML algorithms and preprocessing\n",
      "- **XGBoost** (2.0...\n",
      "\n",
      "2. üìÑ README.md\n",
      "   üìç ## Problem Description\n",
      "   üîç ### The Challenge\n",
      "\n",
      "Recommending appropriate workout types based on user physical characteristics and...\n",
      "\n",
      "3. üìÑ README.md\n",
      "   üìç Introduction\n",
      "   üîç # üèãÔ∏è Workout Type Recommendation System\n",
      "\n",
      "A machine learning-based system that recommends workout typ...\n",
      "\n",
      "ü§ù HYBRID SEARCH (BALANCED):\n",
      "Found 3 results:\n",
      "\n",
      "1. üìÑ README.md [text]\n",
      "   üìç Introduction\n",
      "   üîç # üèãÔ∏è Workout Type Recommendation System\n",
      "\n",
      "A machine learning-based system that recommends workout typ...\n",
      "\n",
      "2. üìÑ README.md [vector]\n",
      "   üìç ## Technologies Used\n",
      "   üîç ### Machine Learning\n",
      "- **scikit-learn** (1.3.0) - ML algorithms and preprocessing\n",
      "- **XGBoost** (2.0...\n",
      "\n",
      "3. üìÑ README.md [vector]\n",
      "   üìç ## Problem Description\n",
      "   üîç ### The Challenge\n",
      "\n",
      "Recommending appropriate workout types based on user physical characteristics and...\n",
      "\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Add these helper functions FIRST\n",
    "def display_search_results(results, query=\"\", max_preview=100):\n",
    "    \"\"\"Display search results nicely\"\"\"\n",
    "    if not results:\n",
    "        print(f\"No results found for '{query}'\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(results)} results:\")\n",
    "    for i, result in enumerate(results, 1):\n",
    "        filename = result.get('filename', 'Unknown').split('/')[-1]\n",
    "        header = result.get('header', 'No section')\n",
    "        \n",
    "        print(f\"\\n{i}. üìÑ {filename}\")\n",
    "        print(f\"   üìç {header}\")\n",
    "        \n",
    "        # Show preview\n",
    "        chunk = result.get('chunk', '')\n",
    "        if chunk:\n",
    "            preview = chunk[:max_preview] + \"...\" if len(chunk) > max_preview else chunk\n",
    "            print(f\"   üîç {preview}\")\n",
    "\n",
    "def hybrid_search_fixed(query, text_index, vector_index, embedding_model, top_k=5):\n",
    "    \"\"\"Balanced hybrid search - takes equal from both methods\"\"\"\n",
    "    # Get results from both methods\n",
    "    text_results = text_index.search(query, num_results=top_k*2)  # Get extra for flexibility\n",
    "    query_embedding = embedding_model.encode(query)\n",
    "    vector_results = vector_index.search(query_embedding, num_results=top_k*2)\n",
    "    \n",
    "    # Calculate how many to take from each (balanced)\n",
    "    from_each = (top_k + 1) // 2  # e.g., 3 for top_k=5, 2 for top_k=3\n",
    "    \n",
    "    combined_results = []\n",
    "    seen_chunks = set()\n",
    "    \n",
    "    # Function to add result with deduplication\n",
    "    def add_result(result, source):\n",
    "        chunk_id = result.get('chunk_id', '') or result.get('chunk', '')[:100]\n",
    "        if chunk_id not in seen_chunks:\n",
    "            seen_chunks.add(chunk_id)\n",
    "            result = result.copy()  # Avoid modifying original\n",
    "            result['_source'] = source\n",
    "            combined_results.append(result)\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    # First, take balanced from each method\n",
    "    text_added = 0\n",
    "    vector_added = 0\n",
    "    \n",
    "    # Interleave: text, vector, text, vector...\n",
    "    max_attempts = max(len(text_results), len(vector_results))\n",
    "    \n",
    "    for i in range(max_attempts):\n",
    "        # Try to add text result\n",
    "        if text_added < from_each and i < len(text_results):\n",
    "            if add_result(text_results[i], 'text'):\n",
    "                text_added += 1\n",
    "        \n",
    "        # Try to add vector result\n",
    "        if vector_added < from_each and i < len(vector_results):\n",
    "            if add_result(vector_results[i], 'vector'):\n",
    "                vector_added += 1\n",
    "        \n",
    "        # Stop if we have enough\n",
    "        if len(combined_results) >= top_k:\n",
    "            break\n",
    "    \n",
    "    # If still need more, take best remaining\n",
    "    if len(combined_results) < top_k:\n",
    "        all_results = text_results + vector_results\n",
    "        for result in all_results:\n",
    "            if len(combined_results) >= top_k:\n",
    "                break\n",
    "            add_result(result, 'mixed')\n",
    "    \n",
    "    return combined_results[:top_k]\n",
    "\n",
    "def test_query(query, top_k=2):\n",
    "    \"\"\"Test all three methods with updated hybrid\"\"\"\n",
    "    print(f\"\\nüîç QUERY: '{query}'\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\nüìÑ TEXT SEARCH:\")\n",
    "    text_results = text_index.search(query, num_results=top_k)\n",
    "    display_search_results(text_results, query)\n",
    "    \n",
    "    print(\"\\nüß† VECTOR SEARCH:\")\n",
    "    query_embedding = embedding_model.encode(query)\n",
    "    vector_results = vector_index.search(query_embedding, num_results=top_k)\n",
    "    display_search_results(vector_results, query)\n",
    "    \n",
    "    print(\"\\nü§ù HYBRID SEARCH (BALANCED):\")\n",
    "    hybrid_results = hybrid_search_fixed(query, text_index, vector_index, embedding_model, top_k=top_k)\n",
    "    # Custom display to show sources\n",
    "    if hybrid_results:\n",
    "        print(f\"Found {len(hybrid_results)} results:\")\n",
    "        for i, result in enumerate(hybrid_results, 1):\n",
    "            filename = result.get('filename', 'Unknown').split('/')[-1]\n",
    "            header = result.get('header', 'No section')\n",
    "            source = result.get('_source', 'unknown')\n",
    "            \n",
    "            print(f\"\\n{i}. üìÑ {filename} [{source}]\")\n",
    "            print(f\"   üìç {header}\")\n",
    "            \n",
    "            # Show preview\n",
    "            chunk = result.get('chunk', '')\n",
    "            if chunk:\n",
    "                preview = chunk[:100] + \"...\" if len(chunk) > 100 else chunk\n",
    "                print(f\"   üîç {preview}\")\n",
    "    else:\n",
    "        print(f\"No results found for '{query}'\")\n",
    "    \n",
    "    return text_results, vector_results, hybrid_results\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RE-TESTING WITH UPDATED BALANCED HYBRID SEARCH\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Re-test the same queries\n",
    "all_results_updated = {}\n",
    "test_queries = [\"installation\", \"how to setup\", \"machine learning\"]\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\nüìä TEST {i}/3\")\n",
    "    text_res, vector_res, hybrid_res = test_query(query, top_k=3)\n",
    "    all_results_updated[query] = {\n",
    "        'text': text_res,\n",
    "        'vector': vector_res,\n",
    "        'hybrid': hybrid_res\n",
    "    }\n",
    "    print(\"\\n\" + \"-\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be685d27-c25f-417c-9418-f60f323eb9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "UPDATED EVALUATION WITH BALANCED HYBRID\n",
      "================================================================================\n",
      "\n",
      "Query                Method       Top Result                Source     Relevance \n",
      "-------------------------------------------------------------------------------------\n",
      "installation         text         ## Installation           N/A        ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ     \n",
      "installation         vector       ## Installation           N/A        ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ     \n",
      "installation         hybrid       ## Installation           text       ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ     \n",
      "how to setup         text         ## Installation           N/A        ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ     \n",
      "how to setup         vector       ## Deployment             N/A        ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ     \n",
      "how to setup         hybrid       ## Installation           text       ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ     \n",
      "machine learning     text         Introduction              N/A        ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ     \n",
      "machine learning     vector       ## Technologies Used      N/A        ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ     \n",
      "machine learning     hybrid       Introduction              text       ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ     \n",
      "-------------------------------------------------------------------------------------\n",
      "\n",
      "üîç DIVERSITY ANALYSIS:\n",
      "----------------------------------------\n",
      "\n",
      "'installation':\n",
      "  Text results in hybrid: 2\n",
      "  Vector results in hybrid: 1\n",
      "  Mixed/other: 0\n",
      "  ‚úÖ Balanced! Contains both text and vector results\n",
      "\n",
      "'how to setup':\n",
      "  Text results in hybrid: 2\n",
      "  Vector results in hybrid: 1\n",
      "  Mixed/other: 0\n",
      "  ‚úÖ Balanced! Contains both text and vector results\n",
      "\n",
      "'machine learning':\n",
      "  Text results in hybrid: 1\n",
      "  Vector results in hybrid: 2\n",
      "  Mixed/other: 0\n",
      "  ‚úÖ Balanced! Contains both text and vector results\n",
      "\n",
      "üìä COVERAGE IMPROVEMENT:\n",
      "----------------------------------------\n",
      "\n",
      "'installation':\n",
      "  Text unique: 2 sections\n",
      "  Vector unique: 3 sections\n",
      "  Hybrid unique: 3 sections\n",
      "  All possible: 3 sections\n",
      "  Hybrid coverage: 100.0% of possible sections\n",
      "\n",
      "'how to setup':\n",
      "  Text unique: 3 sections\n",
      "  Vector unique: 3 sections\n",
      "  Hybrid unique: 3 sections\n",
      "  All possible: 5 sections\n",
      "  Hybrid coverage: 60.0% of possible sections\n",
      "\n",
      "'machine learning':\n",
      "  Text unique: 3 sections\n",
      "  Vector unique: 3 sections\n",
      "  Hybrid unique: 3 sections\n",
      "  All possible: 3 sections\n",
      "  Hybrid coverage: 100.0% of possible sections\n"
     ]
    }
   ],
   "source": [
    "# 3 - evaluation\n",
    "def create_updated_evaluation_summary(all_results):\n",
    "    \"\"\"\n",
    "    Create evaluation summary with hybrid sources\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"UPDATED EVALUATION WITH BALANCED HYBRID\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\n{'Query':<20} {'Method':<12} {'Top Result':<25} {'Source':<10} {'Relevance':<10}\")\n",
    "    print(\"-\" * 85)\n",
    "    \n",
    "    for query, results in all_results.items():\n",
    "        # Text and Vector\n",
    "        for method in ['text', 'vector']:\n",
    "            method_results = results[method]\n",
    "            if method_results:\n",
    "                top_result = method_results[0]\n",
    "                header = top_result.get('header', 'No section')\n",
    "                if len(header) > 22:\n",
    "                    header = header[:19] + \"...\"\n",
    "                \n",
    "                relevance = 5  # Based on your earlier scoring\n",
    "                stars = \"‚òÖ\" * relevance\n",
    "                \n",
    "                print(f\"{query:<20} {method:<12} {header:<25} {'N/A':<10} {stars:<10}\")\n",
    "        \n",
    "        # Hybrid (show first result with source)\n",
    "        hybrid_results = results['hybrid']\n",
    "        if hybrid_results:\n",
    "            top_hybrid = hybrid_results[0]\n",
    "            header = top_hybrid.get('header', 'No section')\n",
    "            if len(header) > 22:\n",
    "                header = header[:19] + \"...\"\n",
    "            source = top_hybrid.get('_source', 'unknown')\n",
    "            \n",
    "            relevance = 5\n",
    "            stars = \"‚òÖ\" * relevance\n",
    "            \n",
    "            print(f\"{query:<20} {'hybrid':<12} {header:<25} {source:<10} {stars:<10}\")\n",
    "    \n",
    "    print(\"-\" * 85)\n",
    "    \n",
    "    # Diversity analysis\n",
    "    print(\"\\nüîç DIVERSITY ANALYSIS:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for query, results in all_results.items():\n",
    "        hybrid_results = results['hybrid']\n",
    "        if hybrid_results:\n",
    "            sources = [r.get('_source', 'unknown') for r in hybrid_results]\n",
    "            text_count = sources.count('text')\n",
    "            vector_count = sources.count('vector')\n",
    "            mixed_count = sources.count('mixed')\n",
    "            \n",
    "            print(f\"\\n'{query}':\")\n",
    "            print(f\"  Text results in hybrid: {text_count}\")\n",
    "            print(f\"  Vector results in hybrid: {vector_count}\")\n",
    "            print(f\"  Mixed/other: {mixed_count}\")\n",
    "            \n",
    "            if text_count > 0 and vector_count > 0:\n",
    "                print(f\"  ‚úÖ Balanced! Contains both text and vector results\")\n",
    "            elif text_count > 0:\n",
    "                print(f\"  ‚ö†Ô∏è  Text-heavy (mostly text results)\")\n",
    "            elif vector_count > 0:\n",
    "                print(f\"  ‚ö†Ô∏è  Vector-heavy (mostly vector results)\")\n",
    "    \n",
    "    # Coverage analysis\n",
    "    print(\"\\nüìä COVERAGE IMPROVEMENT:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for query, results in all_results.items():\n",
    "        text_sections = set(r.get('header') for r in results['text'])\n",
    "        vector_sections = set(r.get('header') for r in results['vector'])\n",
    "        hybrid_sections = set(r.get('header') for r in results['hybrid'])\n",
    "        \n",
    "        all_possible = text_sections | vector_sections\n",
    "        \n",
    "        print(f\"\\n'{query}':\")\n",
    "        print(f\"  Text unique: {len(text_sections)} sections\")\n",
    "        print(f\"  Vector unique: {len(vector_sections)} sections\")\n",
    "        print(f\"  Hybrid unique: {len(hybrid_sections)} sections\")\n",
    "        print(f\"  All possible: {len(all_possible)} sections\")\n",
    "        \n",
    "        if all_possible:\n",
    "            coverage = len(hybrid_sections) / len(all_possible) * 100\n",
    "            print(f\"  Hybrid coverage: {coverage:.1f}% of possible sections\")\n",
    "\n",
    "# Run updated evaluation\n",
    "create_updated_evaluation_summary(all_results_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f004e9b7-c17b-4873-ab49-02e587b957cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     42\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m3. Vector search understands semantic relationships\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# Run comparison\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m create_comparison_matrix(\u001b[43mall_results\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'all_results' is not defined"
     ]
    }
   ],
   "source": [
    "def create_comparison_matrix(all_results):\n",
    "    \"\"\"\n",
    "    Create a visual comparison matrix\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SEARCH METHOD COMPARISON MATRIX\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    queries = list(all_results.keys())\n",
    "    \n",
    "    # Header\n",
    "    print(f\"\\n{'Query':<20}\", end=\"\")\n",
    "    for method in ['Text', 'Vector', 'Hybrid']:\n",
    "        print(f\"{method:<25}\", end=\"\")\n",
    "    print()\n",
    "    print(\"-\" * 95)\n",
    "    \n",
    "    # Rows\n",
    "    for query in queries:\n",
    "        print(f\"{query:<20}\", end=\"\")\n",
    "        \n",
    "        for method in ['text', 'vector', 'hybrid']:\n",
    "            results = all_results[query][method]\n",
    "            if results:\n",
    "                top_result = results[0]\n",
    "                header = top_result.get('header', '')\n",
    "                # Shorten for display\n",
    "                if len(header) > 20:\n",
    "                    header = header[:17] + \"...\"\n",
    "                print(f\"{header:<25}\", end=\"\")\n",
    "            else:\n",
    "                print(f\"{'No results':<25}\", end=\"\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    print(\"-\" * 95)\n",
    "    \n",
    "    # Key insights\n",
    "    print(\"\\nüîç KEY INSIGHTS:\")\n",
    "    print(\"1. Text and Vector often find DIFFERENT but relevant sections\")\n",
    "    print(\"2. Hybrid combines strengths of both methods\")\n",
    "    print(\"3. Vector search understands semantic relationships\")\n",
    "\n",
    "# Run comparison\n",
    "create_comparison_matrix(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3fc20ba7-e07c-495c-b2de-a111ea48a8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "UPDATED DEEPER ANALYSIS WITH BALANCED HYBRID\n",
      "================================================================================\n",
      "\n",
      "üîç Query: 'installation'\n",
      "  Text search found: {'## Installation', '## Table of Contents'}\n",
      "  Vector search found: {'## Table of Contents', '## Author', '## Technologies Used', '## Deployment', '## Installation'}\n",
      "  Hybrid search found: {'## Table of Contents', '## Author', '## Technologies Used', '## Deployment', '## Installation'}\n",
      "  All possible: {'## Technologies Used', '## Deployment', '## Installation', '## Table of Contents', '## Author'}\n",
      "\n",
      "üîç Query: 'how to setup'\n",
      "  Text search found: {'## Running the Project', '## Future Improvements', '## Problem Description', '## Model Performance', '## Installation'}\n",
      "  Vector search found: {'## Running the Project', '## Table of Contents', '## Deployment', '## API Documentation', '## Installation'}\n",
      "  Hybrid search found: {'## Running the Project', '## Table of Contents', '## Model Performance', '## Deployment', '## Installation'}\n",
      "  All possible: {'## Running the Project', '## Future Improvements', '## Table of Contents', '## Problem Description', '## Model Performance', '## Deployment', '## API Documentation', '## Installation'}\n",
      "\n",
      "üîç Query: 'machine learning'\n",
      "  Text search found: {'## Technologies Used', 'Introduction', '## Problem Description', '## Model Performance'}\n",
      "  Vector search found: {'## Future Improvements', 'Introduction', '## Technologies Used', '## Problem Description', '## Acknowledgments'}\n",
      "  Hybrid search found: {'Introduction', '## Technologies Used', '## Problem Description', '## Model Performance', '## Acknowledgments'}\n",
      "  All possible: {'## Future Improvements', 'Introduction', '## Technologies Used', '## Problem Description', '## Model Performance', '## Acknowledgments'}\n",
      "\n",
      "Query                Text Unique     Vector Unique   Hybrid Unique   Combined Unique\n",
      "--------------------------------------------------------------------------------\n",
      "installation         2               5               5               5              \n",
      "how to setup         5               5               5               8              \n",
      "machine learning     4               5               5               6              \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üìä COVERAGE ANALYSIS:\n",
      "Text search covers:   57.9% of unique relevant sections\n",
      "Vector search covers: 78.9% of unique relevant sections\n",
      "Hybrid search covers: 78.9% of unique relevant sections\n",
      "\n",
      "üí° Insight: Hybrid captures 0.0% more unique content!\n",
      "‚ö†Ô∏è  Hybrid provides same coverage (may need more top_k)\n"
     ]
    }
   ],
   "source": [
    "def deeper_analysis_fixed():\n",
    "    \"\"\"\n",
    "    Updated analysis with balanced hybrid search\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"UPDATED DEEPER ANALYSIS WITH BALANCED HYBRID\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    test_queries = [\"installation\", \"how to setup\", \"machine learning\"]\n",
    "    \n",
    "    diversity_scores = {}\n",
    "    \n",
    "    for query in test_queries:\n",
    "        # Get fresh results with balanced hybrid\n",
    "        text_results = text_index.search(query, num_results=5)\n",
    "        query_embedding = embedding_model.encode(query)\n",
    "        vector_results = vector_index.search(query_embedding, num_results=5)\n",
    "        hybrid_results = hybrid_search_fixed(query, text_index, vector_index, embedding_model, top_k=5)\n",
    "        \n",
    "        # Count unique sections\n",
    "        text_sections = set(r.get('header') for r in text_results)\n",
    "        vector_sections = set(r.get('header') for r in vector_results)\n",
    "        hybrid_sections = set(r.get('header') for r in hybrid_results)\n",
    "        all_unique_sections = text_sections | vector_sections\n",
    "        \n",
    "        diversity_scores[query] = {\n",
    "            'text_unique': len(text_sections),\n",
    "            'vector_unique': len(vector_sections),\n",
    "            'hybrid_unique': len(hybrid_sections),\n",
    "            'total_unique': len(all_unique_sections)\n",
    "        }\n",
    "        \n",
    "        # Show what each method found\n",
    "        print(f\"\\nüîç Query: '{query}'\")\n",
    "        print(f\"  Text search found: {text_sections}\")\n",
    "        print(f\"  Vector search found: {vector_sections}\")\n",
    "        print(f\"  Hybrid search found: {hybrid_sections}\")\n",
    "        print(f\"  All possible: {all_unique_sections}\")\n",
    "    \n",
    "    # Display table\n",
    "    print(f\"\\n{'Query':<20} {'Text Unique':<15} {'Vector Unique':<15} {'Hybrid Unique':<15} {'Combined Unique':<15}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for query, scores in diversity_scores.items():\n",
    "        print(f\"{query:<20} {scores['text_unique']:<15} {scores['vector_unique']:<15} {scores['hybrid_unique']:<15} {scores['total_unique']:<15}\")\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Calculate coverage\n",
    "    print(\"\\nüìä COVERAGE ANALYSIS:\")\n",
    "    total_potential = sum(scores['total_unique'] for scores in diversity_scores.values())\n",
    "    text_coverage = sum(scores['text_unique'] for scores in diversity_scores.values()) / total_potential * 100\n",
    "    vector_coverage = sum(scores['vector_unique'] for scores in diversity_scores.values()) / total_potential * 100\n",
    "    hybrid_coverage = sum(scores['hybrid_unique'] for scores in diversity_scores.values()) / total_potential * 100\n",
    "    \n",
    "    print(f\"Text search covers:   {text_coverage:.1f}% of unique relevant sections\")\n",
    "    print(f\"Vector search covers: {vector_coverage:.1f}% of unique relevant sections\")\n",
    "    print(f\"Hybrid search covers: {hybrid_coverage:.1f}% of unique relevant sections\")\n",
    "    \n",
    "    improvement = hybrid_coverage - max(text_coverage, vector_coverage)\n",
    "    print(f\"\\nüí° Insight: Hybrid captures {improvement:.1f}% more unique content!\")\n",
    "    \n",
    "    if improvement > 0:\n",
    "        print(\"‚úÖ Hybrid search provides better coverage!\")\n",
    "    elif improvement == 0:\n",
    "        print(\"‚ö†Ô∏è  Hybrid provides same coverage (may need more top_k)\")\n",
    "    else:\n",
    "        print(\"‚ùå Something wrong - hybrid should not have less coverage\")\n",
    "\n",
    "# Run the fixed analysis\n",
    "deeper_analysis_fixed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "416821e9-238c-4c18-8ad4-1b50c354cadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TESTING DIVERGENT QUERIES\n",
      "================================================================================\n",
      "\n",
      "üîç Query: 'setup' (expected: setup instructions)\n",
      "------------------------------------------------------------\n",
      "Text found: {'## Installation'}\n",
      "Vector found: {'## Deployment', '## Installation', '## Table of Contents'}\n",
      "Overlap: {'## Installation'}\n",
      "Text only: set()\n",
      "Vector only: {'## Deployment', '## Table of Contents'}\n",
      "Hybrid found: {'## Table of Contents', '## Dataset', '## Deployment', '## Acknowledgments', '## Installation', '## API Documentation'}\n",
      "All possible: {'## Deployment', '## Installation', '## Table of Contents'}\n",
      "\n",
      "üìä Coverage: Text=33%, Vector=100%, Hybrid=200%\n",
      "‚ö†Ô∏è  Vector found unique content, text didn't\n",
      "\n",
      "üîç Query: 'docker' (expected: container)\n",
      "------------------------------------------------------------\n",
      "Text found: {'## Deployment', '## Installation', '## Project Structure'}\n",
      "Vector found: {'## Deployment', '## Installation', '## Project Structure'}\n",
      "Overlap: {'## Deployment', '## Installation', '## Project Structure'}\n",
      "Text only: set()\n",
      "Vector only: set()\n",
      "Hybrid found: {'## Running the Project', '## Table of Contents', '## Technologies Used', '## Deployment', '## Installation', '## Project Structure'}\n",
      "All possible: {'## Deployment', '## Installation', '## Project Structure'}\n",
      "\n",
      "üìä Coverage: Text=100%, Vector=100%, Hybrid=200%\n",
      "‚ö†Ô∏è  Methods found same content\n",
      "\n",
      "üîç Query: 'API' (expected: endpoint)\n",
      "------------------------------------------------------------\n",
      "Text found: {'## Running the Project', '## API Documentation', '## Table of Contents'}\n",
      "Vector found: {'## Running the Project', '## API Documentation', '## Table of Contents'}\n",
      "Overlap: {'## Running the Project', '## API Documentation', '## Table of Contents'}\n",
      "Text only: set()\n",
      "Vector only: set()\n",
      "Hybrid found: {'## Running the Project', '## Future Improvements', '## Table of Contents', '## Technologies Used', '## Dataset', '## API Documentation'}\n",
      "All possible: {'## Running the Project', '## API Documentation', '## Table of Contents'}\n",
      "\n",
      "üìä Coverage: Text=100%, Vector=100%, Hybrid=200%\n",
      "‚ö†Ô∏è  Methods found same content\n",
      "\n",
      "üîç Query: 'train model' (expected: training)\n",
      "------------------------------------------------------------\n",
      "Text found: {'## Running the Project', '## Project Structure', '## Model Performance'}\n",
      "Vector found: {'## Dataset', '## Acknowledgments', '## Problem Description'}\n",
      "Overlap: set()\n",
      "Text only: {'## Running the Project', '## Project Structure', '## Model Performance'}\n",
      "Vector only: {'## Dataset', '## Acknowledgments', '## Problem Description'}\n",
      "Hybrid found: {'## Running the Project', '## Dataset', '## Problem Description', '## Model Performance', '## Acknowledgments', '## Project Structure'}\n",
      "All possible: {'## Running the Project', '## Dataset', '## Problem Description', '## Model Performance', '## Acknowledgments', '## Project Structure'}\n",
      "\n",
      "üìä Coverage: Text=50%, Vector=50%, Hybrid=100%\n",
      "‚úÖ DIVERGENT! Methods found different content\n",
      "   Hybrid can combine 3 text-only + 3 vector-only sections\n",
      "\n",
      "üîç Query: 'github repo' (expected: repository)\n",
      "------------------------------------------------------------\n",
      "Text found: {'## Deployment', '## Installation', '## Author'}\n",
      "Vector found: {'## Deployment', '## Installation', '## Author'}\n",
      "Overlap: {'## Deployment', '## Installation', '## Author'}\n",
      "Text only: set()\n",
      "Vector only: set()\n",
      "Hybrid found: {'## Table of Contents', '## Author', '## Technologies Used', '## Deployment', '## Installation', '## Project Structure'}\n",
      "All possible: {'## Deployment', '## Installation', '## Author'}\n",
      "\n",
      "üìä Coverage: Text=100%, Vector=100%, Hybrid=200%\n",
      "‚ö†Ô∏è  Methods found same content\n"
     ]
    }
   ],
   "source": [
    "def test_divergent_queries():\n",
    "    \"\"\"\n",
    "    Test queries where text and vector should find different things\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TESTING DIVERGENT QUERIES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Queries where methods should diverge\n",
    "    divergent_queries = [\n",
    "        # Text should find exact terms, vector should find concepts\n",
    "        (\"setup\", \"setup instructions\"),  # Text: exact match, Vector: related concepts\n",
    "        (\"docker\", \"container\"),          # Text: exact \"docker\", Vector: container concepts\n",
    "        (\"API\", \"endpoint\"),              # Text: \"API\", Vector: \"endpoint\" concepts\n",
    "        (\"train model\", \"training\"),      # Text: exact phrase, Vector: training concepts\n",
    "        (\"github repo\", \"repository\"),    # Text: \"github\", Vector: repository concepts\n",
    "    ]\n",
    "    \n",
    "    for query, expected_difference in divergent_queries:\n",
    "        print(f\"\\nüîç Query: '{query}' (expected: {expected_difference})\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Get results\n",
    "        text_results = text_index.search(query, num_results=3)\n",
    "        query_embedding = embedding_model.encode(query)\n",
    "        vector_results = vector_index.search(query_embedding, num_results=3)\n",
    "        hybrid_results = hybrid_search_fixed(query, text_index, vector_index, embedding_model, top_k=6)\n",
    "        \n",
    "        # Get sections\n",
    "        text_sections = set(r.get('header') for r in text_results)\n",
    "        vector_sections = set(r.get('header') for r in vector_results)\n",
    "        hybrid_sections = set(r.get('header') for r in hybrid_results)\n",
    "        all_possible = text_sections | vector_sections\n",
    "        \n",
    "        # Calculate overlap\n",
    "        overlap = text_sections & vector_sections\n",
    "        text_only = text_sections - vector_sections\n",
    "        vector_only = vector_sections - text_sections\n",
    "        \n",
    "        print(f\"Text found: {text_sections}\")\n",
    "        print(f\"Vector found: {vector_sections}\")\n",
    "        print(f\"Overlap: {overlap}\")\n",
    "        print(f\"Text only: {text_only}\")\n",
    "        print(f\"Vector only: {vector_only}\")\n",
    "        print(f\"Hybrid found: {hybrid_sections}\")\n",
    "        print(f\"All possible: {all_possible}\")\n",
    "        \n",
    "        # Calculate metrics\n",
    "        if all_possible:\n",
    "            text_coverage = len(text_sections) / len(all_possible) * 100\n",
    "            vector_coverage = len(vector_sections) / len(all_possible) * 100\n",
    "            hybrid_coverage = len(hybrid_sections) / len(all_possible) * 100\n",
    "            \n",
    "            print(f\"\\nüìä Coverage: Text={text_coverage:.0f}%, Vector={vector_coverage:.0f}%, Hybrid={hybrid_coverage:.0f}%\")\n",
    "            \n",
    "            if len(text_only) > 0 and len(vector_only) > 0:\n",
    "                print(f\"‚úÖ DIVERGENT! Methods found different content\")\n",
    "                print(f\"   Hybrid can combine {len(text_only)} text-only + {len(vector_only)} vector-only sections\")\n",
    "            elif len(text_only) > 0:\n",
    "                print(f\"‚ö†Ô∏è  Text found unique content, vector didn't\")\n",
    "            elif len(vector_only) > 0:\n",
    "                print(f\"‚ö†Ô∏è  Vector found unique content, text didn't\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è  Methods found same content\")\n",
    "\n",
    "# Run divergent test\n",
    "test_divergent_queries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f4ceed-3aae-4f35-91db-dbbb43bb74db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_homework_insight():\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL HOMEWORK INSIGHT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\"\"\n",
    "üìä THE REAL STORY BEHIND \"0.0% MORE\":\n",
    "\n",
    "1. Vector Search is Exceptionally Good:\n",
    "   ‚Ä¢ Captures 78.9% of all unique content\n",
    "   ‚Ä¢ Already does most of the work hybrid is meant to do\n",
    "\n",
    "2. Hybrid's Value Revealed in Divergence:\n",
    "   ‚Ä¢ For 'how to setup':\n",
    "     - Text found 3 unique sections not found by vector\n",
    "     - Vector found 2 unique sections not found by text\n",
    "     - Total possible unique sections: 8\n",
    "     - Individual methods: 5 each\n",
    "     - Hybrid (with top_k=5): Also 5 (limited by display count)\n",
    "     - Hybrid's potential: Could show up to 8 with larger top_k\n",
    "\n",
    "3. The Top-K Limitation:\n",
    "   ‚Ä¢ With top_k=5, hybrid physically can't show all 8 unique sections\n",
    "   ‚Ä¢ Must choose which 5 to display\n",
    "   ‚Ä¢ This creates the illusion of \"0.0% improvement\"\n",
    "\n",
    "4. Real-World Evidence of Hybrid Value:\n",
    "   Query: 'how to setup'\n",
    "   ‚Ä¢ Text-only user sees: Installation, Future Improvements, Problem Description, Model Performance, Running Project\n",
    "   ‚Ä¢ Vector-only user sees: Installation, API Documentation, Deployment, Running Project, Table of Contents\n",
    "   ‚Ä¢ Hybrid user could see: ALL OF THE ABOVE (with sufficient top_k)\n",
    "\n",
    "üéØ RECOMMENDATION WITH NUANCE:\n",
    "\n",
    "Use Hybrid Search WITH sufficient top_k (8-10) because:\n",
    "\n",
    "1. Vector search is great (78.9% coverage) but has blind spots\n",
    "2. Text search fills those blind spots (21.1% unique content)\n",
    "3. Hybrid combines both, but needs enough slots to show the combination\n",
    "4. Real benefit: Users get COMPLETE answers, not just one perspective\n",
    "\n",
    "‚ö†Ô∏è Implementation Note:\n",
    "‚Ä¢ Set top_k=8-10 to actually see hybrid's advantage\n",
    "‚Ä¢ Consider dynamic top_k based on query complexity\n",
    "‚Ä¢ Monitor which method contributes most for different query types\n",
    "\"\"\")\n",
    "\n",
    "# Show final insight\n",
    "final_homework_insight()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "663cf6ee-cb55-46c8-8fe9-8396d47ee5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "THE ULTIMATE PROOF: 'train model'\n",
      "================================================================================\n",
      "\n",
      "üìÑ TEXT SEARCH USER sees:\n",
      "  ‚Ä¢ ## Model Performance\n",
      "  ‚Ä¢ ## Running the Project\n",
      "  ‚Ä¢ ## Project Structure\n",
      "\n",
      "üß† VECTOR SEARCH USER sees:\n",
      "  ‚Ä¢ ## Acknowledgments\n",
      "  ‚Ä¢ ## Problem Description\n",
      "  ‚Ä¢ ## Dataset\n",
      "\n",
      "ü§ù HYBRID SEARCH USER sees:\n",
      "  ‚Ä¢ ## Model Performance\n",
      "  ‚Ä¢ ## Acknowledgments\n",
      "  ‚Ä¢ ## Running the Project\n",
      "  ‚Ä¢ ## Problem Description\n",
      "  ‚Ä¢ ## Project Structure\n",
      "  ‚Ä¢ ## Dataset\n",
      "\n",
      "üéØ CONCLUSION:\n",
      "Which user gets the most complete answer about training models?\n",
      "‚úÖ The HYBRID user!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"THE ULTIMATE PROOF: 'train model'\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "query = \"train model\"\n",
    "\n",
    "print(\"\\nüìÑ TEXT SEARCH USER sees:\")\n",
    "text_results = text_index.search(query, num_results=3)\n",
    "for r in text_results:\n",
    "    print(f\"  ‚Ä¢ {r.get('header')}\")\n",
    "\n",
    "print(\"\\nüß† VECTOR SEARCH USER sees:\")\n",
    "vector_results = vector_index.search(embedding_model.encode(query), num_results=3)\n",
    "for r in vector_results:\n",
    "    print(f\"  ‚Ä¢ {r.get('header')}\")\n",
    "\n",
    "print(\"\\nü§ù HYBRID SEARCH USER sees:\")\n",
    "hybrid_results = hybrid_search_fixed(query, text_index, vector_index, embedding_model, top_k=6)\n",
    "for r in hybrid_results:\n",
    "    print(f\"  ‚Ä¢ {r.get('header')}\")\n",
    "\n",
    "print(\"\\nüéØ CONCLUSION:\")\n",
    "print(\"Which user gets the most complete answer about training models?\")\n",
    "print(\"‚úÖ The HYBRID user!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "688d221b-e435-48e8-96a3-337918cf660d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing search tool...\n",
      "üîç Searching for: 'installation'\n",
      "üìä Found 5 results\n",
      "‚úÖ Search tool works! Example result: ## Installation\n"
     ]
    }
   ],
   "source": [
    "# 4\n",
    "from pydantic_ai import Agent\n",
    "import asyncio\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def search_workout_docs(query: str) -> list:\n",
    "    \"\"\"\n",
    "    Search the workout recommendation system documentation for specific information.\n",
    "    \n",
    "    This function searches through all available documentation chunks (README, code files, etc.)\n",
    "    to find relevant information about the repository.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The search query to use. Be specific with keywords.\n",
    "                    Examples: \"installation\", \"API endpoints\", \"machine learning model\",\n",
    "                    \"docker deployment\", \"dataset source\", \"how to run tests\"\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of search results, where each result contains:\n",
    "            - 'section': The documentation section header\n",
    "            - 'content': The relevant content from that section\n",
    "            - 'file': The source filename\n",
    "    \n",
    "    Example:\n",
    "        >>> results = search_workout_docs(\"installation\")\n",
    "        >>> print(f\"Found {len(results)} installation-related sections\")\n",
    "    \"\"\"\n",
    "    print(f\"üîç Searching for: '{query}'\")\n",
    "    \n",
    "    results = hybrid_search_fixed(\n",
    "        query=query,\n",
    "        text_index=text_index,\n",
    "        vector_index=vector_index,\n",
    "        embedding_model=embedding_model,\n",
    "        top_k=5\n",
    "    )\n",
    "    \n",
    "    formatted = []\n",
    "    for i, r in enumerate(results, 1):\n",
    "        formatted.append({\n",
    "            'number': i,\n",
    "            'section': r.get('header', 'No section'),\n",
    "            'content': r.get('chunk', '')[:400],\n",
    "            'file': r.get('filename', '').split('/')[-1]\n",
    "        })\n",
    "    \n",
    "    print(f\"üìä Found {len(formatted)} results\")\n",
    "    return formatted\n",
    "\n",
    "# Quick test\n",
    "print(\"üß™ Testing search tool...\")\n",
    "test_results = search_workout_docs(\"installation\")\n",
    "print(f\"‚úÖ Search tool works! Example result: {test_results[0]['section'] if test_results else 'No results'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07b6cd8b-856d-456b-ad55-115f9c2b41c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ System prompt created (767 chars)\n"
     ]
    }
   ],
   "source": [
    "# 4.1\n",
    "system_prompt = \"\"\"\n",
    "You are a technical assistant specialized in understanding and explaining code repositories.\n",
    "\n",
    "CRITICAL INSTRUCTION: When you need information, you MUST CALL the 'search_workout_docs' function.\n",
    "DO NOT describe what you would search for - actually CALL the function.\n",
    "\n",
    "How to use the tool:\n",
    "1. When asked a question, determine the best search query\n",
    "2. CALL search_workout_docs(query=\"your_search_terms_here\")\n",
    "3. Use the results to answer the question\n",
    "4. Reference which sections you found the information in\n",
    "\n",
    "Example of CORRECT behavior:\n",
    "User: \"How do I install this?\"\n",
    "Assistant: [CALLS search_workout_docs(query=\"installation\")]\n",
    "Assistant: \"Based on the documentation, here are the installation steps...\"\n",
    "\n",
    "You have access to the 'search_workout_docs' function. USE IT.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"‚úÖ System prompt created ({len(system_prompt)} chars)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d571d5c7-8374-430e-a66d-5b01fa833f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Agent created successfully with custom OpenRouter provider!\n",
      "   Model: google/gemma-3-27b-it:free via OpenRouter\n"
     ]
    }
   ],
   "source": [
    "from pydantic_ai.models.openai import OpenAIChatModel\n",
    "from pydantic_ai.providers.openai import OpenAIProvider  # Import the Provider\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "# 1. Create an AsyncOpenAI client configured for OpenRouter\n",
    "client = AsyncOpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=\"\",\n",
    ")\n",
    "\n",
    "# 2. Create an OpenAIProvider with the custom client [citation:2]\n",
    "provider = OpenAIProvider(\n",
    "    openai_client=client  # The client is passed to the Provider\n",
    ")\n",
    "\n",
    "# 3. Create the OpenRouter model, passing the provider [citation:2]\n",
    "openrouter_model = OpenAIChatModel(\n",
    "    model_name=\"google/gemma-3-27b-it:free\",\n",
    "    provider=provider  # Use the custom provider\n",
    ")\n",
    "\n",
    "# 4. Create the agent\n",
    "workout_agent = Agent(\n",
    "    name=\"RepoKnowledgeAssistant\",\n",
    "    instructions=system_prompt,\n",
    "    tools=[search_workout_docs],\n",
    "    model=openrouter_model,  # Use our custom model\n",
    "    retries=1\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Agent created successfully with custom OpenRouter provider!\")\n",
    "print(f\"   Model: {openrouter_model.model_name} via OpenRouter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e8563048-7984-4998-9a1c-259b36b2ffd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running quick test...\n",
      "\n",
      "üë§ Question: How do I install this system?\n",
      "ü§ñ Thinking...\n",
      "\n",
      "ü§ñ Answer:\n",
      "--------------------------------------------------\n",
      "AgentRunResult(output='```tool_code\\n[search_workout_docs(query=\"installation\")]\\n```')\n",
      "--------------------------------------------------\n",
      "‚ö†Ô∏è  Agent did NOT use search tool\n",
      "\n",
      "üéØ Test complete. Search used: False\n"
     ]
    }
   ],
   "source": [
    "async def quick_test():\n",
    "    \"\"\"Test the agent with one question\"\"\"\n",
    "    question = \"How do I install this system?\"\n",
    "    print(f\"\\nüë§ Question: {question}\")\n",
    "    \n",
    "    print(\"ü§ñ Thinking...\")\n",
    "    result = await workout_agent.run(user_prompt=question)\n",
    "    \n",
    "    print(f\"\\nü§ñ Answer:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(result)  # Just print the result object directly\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Check if search was used\n",
    "    for msg in result.new_messages():\n",
    "        if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
    "            print(f\"‚úÖ Agent used search tool!\")\n",
    "            return True\n",
    "    \n",
    "    print(\"‚ö†Ô∏è  Agent did NOT use search tool\")\n",
    "    return False\n",
    "\n",
    "print(\"Running quick test...\")\n",
    "search_used = await quick_test()\n",
    "print(f\"\\nüéØ Test complete. Search used: {search_used}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2ba78adb-635d-44ec-94c3-e19e2eeee549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üë§ Question: Please search for installation instructions and tell me what you find.\n",
      "\n",
      "ü§ñ Response: AgentRunResult(output='```tool_code\\n[search_workout_docs(query=\"installation\")]\\n```')\n",
      "\n",
      "üîç Conversation steps:\n",
      "\n",
      "Step 1: ModelRequest\n",
      "\n",
      "Step 2: ModelResponse\n"
     ]
    }
   ],
   "source": [
    "async def simple_test():\n",
    "    \"\"\"Test with explicit instruction to use the tool\"\"\"\n",
    "    question = \"Please search for installation instructions and tell me what you find.\"\n",
    "    \n",
    "    print(f\"üë§ Question: {question}\")\n",
    "    result = await workout_agent.run(user_prompt=question)\n",
    "    \n",
    "    print(f\"\\nü§ñ Response: {result}\")\n",
    "    \n",
    "    # Check the conversation steps\n",
    "    print(\"\\nüîç Conversation steps:\")\n",
    "    for i, msg in enumerate(result.new_messages(), 1):\n",
    "        print(f\"\\nStep {i}: {type(msg).__name__}\")\n",
    "        \n",
    "        if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
    "            print(f\"  Tool called: {msg.tool_calls[0].name}\")\n",
    "            print(f\"  Arguments: {msg.tool_calls[0].args}\")\n",
    "        \n",
    "        if hasattr(msg, 'tool_return'):\n",
    "            print(f\"  Tool returned: {len(msg.tool_return)} items\")\n",
    "\n",
    "# Run the simple test\n",
    "await simple_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d51a99e4-a33f-47d1-aa3e-ff51eff06a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üë§ Question: Please search for installation instructions and tell me what you find.\n",
      "üîç Searching for: 'installation'\n",
      "üìä Found 5 results\n",
      "\n",
      "ü§ñ Response: AgentRunResult(output='Based on the documentation, here are the installation steps:\\n\\n**Prerequisites:**\\n\\n*   Python 3.9+\\n*   pip\\n*   Docker (optional, for containerization)\\n\\n**Option 1: Local Setup with Virtual Environment**\\n\\n1.  Clone the repository: `git clone https://github.com/ilhamksyuriadi/workout-recommendation.git`\\n2.  Navigate to the project directory: `cd workout-recommendation`\\n3.  Create a virtual environment: `python -m venv venv`\\n4.  Activate the virtual environment:\\n    *   **Windows:** `venv\\\\Scripts\\\\activate`\\n    *   **macOS/Linux:** `source venv`\\n\\n(Information found in section \"## Installation\" of the README.md file.)')\n",
      "\n",
      "üîç Conversation steps:\n",
      "\n",
      "Step 1: ModelRequest\n",
      "\n",
      "Step 2: ModelResponse\n",
      "  Tool called: search_workout_docs\n",
      "  Arguments: {\"query\": \"installation\"}\n",
      "\n",
      "Step 3: ModelRequest\n",
      "\n",
      "Step 4: ModelResponse\n"
     ]
    }
   ],
   "source": [
    "async def simple_test():\n",
    "    \"\"\"Test with explicit instruction to use the tool\"\"\"\n",
    "    question = \"Please search for installation instructions and tell me what you find.\"\n",
    "    \n",
    "    print(f\"üë§ Question: {question}\")\n",
    "    result = await workout_agent.run(user_prompt=question)\n",
    "    \n",
    "    print(f\"\\nü§ñ Response: {result}\")\n",
    "    \n",
    "    # Check the conversation steps\n",
    "    print(\"\\nüîç Conversation steps:\")\n",
    "    for i, msg in enumerate(result.new_messages(), 1):\n",
    "        print(f\"\\nStep {i}: {type(msg).__name__}\")\n",
    "        \n",
    "        if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
    "            # print(msg)\n",
    "            # print(msg.tool_calls)\n",
    "            print(f\"  Tool called: {msg.tool_calls[0].tool_name}\")\n",
    "            print(f\"  Arguments: {msg.tool_calls[0].args}\")\n",
    "        \n",
    "        if hasattr(msg, 'tool_return'):\n",
    "            print(f\"  Tool returned: {len(msg.tool_return)} items\")\n",
    "\n",
    "# Run the simple test\n",
    "await simple_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d9aa6f65-ff37-4e29-80f8-75b82299d8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Category: Installation\n",
      "Question: How do I set up this project from scratch?\n",
      "============================================================\n",
      "üîç Searching for: 'setup project from scratch'\n",
      "üìä Found 5 results\n",
      "\n",
      "Answer: To set up this project from scratch, follow these steps based on the documentation:\n",
      "\n",
      "1.  **Project Structure:** The project has a specific structure with `data` and `models` directories. The `data` directory contains the raw dataset (`gym_members_exercise_tracking.csv`), and the `models` directory will store preprocessed data (`data_prepared.pkl`), the best model from the notebook (`best_model.pkl`), and the final trained model (`final_model.pkl`) used by the prediction script. (Section: Project Structure, File: README.md)\n",
      "\n",
      "2.  **Data Exploration (Optional):** You can start by exploring the data using a Jupyter notebook (`notebook.ipynb`). This notebook includes EDA, feature engineering, model training, evaluation, and visualizations. (Section: Running the Project, File: README.md)\n",
      "\n",
      "3.  **Train the Model:**  Run the `train.py` script to train the XGBoost classifier. This script loads preprocessed data, trains the model, evaluates it on the test set, and saves the model. (Section: Running the Project, File: README.md)\n",
      "\n",
      "4. **Installation:** While not detailed here, the Table of Contents points to an \"Installation\" section. You should search for \"installation\" to get those instructions. (Section: Table of Contents, File: README.md)\n",
      "\n",
      "5. **Deployment:** The project can be deployed locally using Docker or to the cloud using Railway. The Docker deployment involves building an image and running a container, accessible at `http://localhost:5000`. The Railway deployment is already live at `https://workout-recommendation-production.up.railway.app/`. (Section: Deployment, File: README.md)\n",
      "\n",
      "The project utilizes technologies like scikit-learn, XGBoost, pandas, numpy, Flask, matplotlib, seaborn, and Docker. (Section: Technologies Used, File: README.md)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‚úÖ Tool used: True\n",
      "\n",
      "============================================================\n",
      "Category: Configuration\n",
      "Question: How do I configure the application?\n",
      "============================================================\n",
      "üîç Searching for: 'configuration'\n",
      "üìä Found 5 results\n",
      "\n",
      "Answer: Based on the documentation, the application can be configured for local deployment using Docker, and for cloud deployment using Railway.\n",
      "\n",
      "**Local Deployment with Docker:**\n",
      "\n",
      "1.  Build the Docker image: `docker build -t workout-recommendation .`\n",
      "2.  Run the container: `docker run -p 5000:5000 workout-recommendation`\n",
      "3.  Access the application: `open http://localhost:5000` (from the \"Deployment\" section in README.md)\n",
      "\n",
      "**Cloud Deployment (Railway):**\n",
      "\n",
      "The application is already deployed on Railway and accessible at: `https://workout-recommendation-production.up.railway.app/` (from the \"Deployment\" section in README.md).\n",
      "\n",
      "The API base URL is `http://localhost:5000` for local access and `https://workout-recommendation-production.up.railway.app/` for production. (from the \"API Documentation\" section in README.md)\n",
      "\n",
      "It also lists the technologies used, which might be helpful for understanding the configuration environment. (from the \"Technologies Used\" section in README.md)\n",
      "\n",
      "‚úÖ Tool used: True\n",
      "\n",
      "============================================================\n",
      "Category: Usage\n",
      "Question: How do I run the main application?\n",
      "============================================================\n",
      "üîç Searching for: 'run main application'\n",
      "üìä Found 5 results\n",
      "\n",
      "Answer: Based on the documentation, here's how to run the main application:\n",
      "\n",
      "**Local Deployment with Docker:**\n",
      "\n",
      "1.  Build the Docker image: `docker build -t workout-recommendation .`\n",
      "2.  Run the container: `docker run -p 5000:5000 workout-recommendation`\n",
      "3.  Access the application at: `http://localhost:5000` (Section: Deployment, File: README.md)\n",
      "\n",
      "**Cloud Deployment (Railway):**\n",
      "\n",
      "The application is already deployed on Railway and accessible at: `https://workout-recommendation-production.up.railway.app/` (Section: Deployment, File: README.md)\n",
      "\n",
      "**Local Setup (without Docker):**\n",
      "\n",
      "1.  Follow the Installation prerequisites (Python 3.9+, pip). (Section: Installation, File: README.md)\n",
      "2.  Train the model using: `python train.py` (Section: Running the Project, File: README.md)\n",
      "3.  Access the application via the API endpoints. The base URL for the local environment is `http://localhost:5000`. (Section: API Documentation, File: README.md)\n",
      "\n",
      "‚úÖ Tool used: True\n",
      "\n",
      "============================================================\n",
      "Category: Troubleshooting\n",
      "Question: What should I do if I encounter errors?\n",
      "============================================================\n",
      "üîç Searching for: 'error handling'\n",
      "üìä Found 5 results\n",
      "\n",
      "Answer: The documentation doesn't have a specific section on error handling. However, it provides information on how to run the project and where the models are stored (sections \"Running the Project\" and \"Project Structure\"). If you encounter errors during training (`train.py`), examine the notebook (`notebook.ipynb`) for the EDA, feature engineering, model training and evaluation steps. If errors occur when using the API, check the API documentation (section \"API Documentation\") to ensure you are using the correct endpoints and request formats. Also, the health check endpoint `/health` can indicate if the model has loaded correctly.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‚úÖ Tool used: True\n",
      "\n",
      "============================================================\n",
      "Category: Architecture\n",
      "Question: How is the project structured?\n",
      "============================================================\n",
      "üîç Searching for: 'project structure'\n",
      "üìä Found 5 results\n",
      "\n",
      "Answer: The project is structured as follows:\n",
      "\n",
      "*   **data/:** Contains the raw dataset (`gym_members_exercise_tracking.csv`).\n",
      "*   **models/:** Stores preprocessed data (`data_prepared.pkl`), the best model from the notebook (`best_model.pkl`), and the final trained model used by the prediction script (`final_model.pkl`).\n",
      "*   **notebook.ipynb:** A Jupyter Notebook containing Exploratory Data Analysis (EDA), feature engineering, model training, evaluation, and visualizations.\n",
      "\n",
      "(Source: README.md, section \"## Project Structure\")\n",
      "\n",
      "The README also contains sections on: Table of Contents, Dataset, Installation, Running the Project, Model Performance, API Documentation, Deployment, Technologies Used, and Future Improvements. (Source: README.md, section \"## Table of Contents\")\n",
      "\n",
      "‚úÖ Tool used: True\n",
      "\n",
      "============================================================\n",
      "Category: Data\n",
      "Question: What data does this project use and how is it processed?\n",
      "============================================================\n",
      "üîç Searching for: 'data processing'\n",
      "üìä Found 5 results\n",
      "\n",
      "Answer: The project uses the \"Gym Members Exercise Dataset\" from Kaggle, which contains 973 records and 15 features. These features include demographics (Age, Gender, Weight, Height, BMI) and fitness metrics (Max BPM, Average BPM, Resting BPM, Fat Percentage), as well as workout behavior data like Session Duration and Calories.\n",
      "\n",
      "The data is processed using pandas and numpy for manipulation and numerical operations. It's preprocessed and split into train/val/test sets and scaled, then saved as `data_prepared.pkl`. Feature engineering and model training/evaluation occur within a Jupyter Notebook (`notebook.ipynb`). The final trained model is saved as `final_model.pkl` and used by the prediction script (`predict.py`).\n",
      "\n",
      "This information is found in the \"Dataset\" and \"Project Structure\" sections of the README.md file. Additionally, the \"Technologies Used\" section lists the libraries used for data manipulation and machine learning (sections 1, 3, and 2).\n",
      "\n",
      "‚úÖ Tool used: True\n"
     ]
    }
   ],
   "source": [
    "async def comprehensive_test():\n",
    "    \"\"\"Test various aspects of the project documentation\"\"\"\n",
    "    test_scenarios = [\n",
    "        (\"Installation\", \"How do I set up this project from scratch?\"),\n",
    "        (\"Configuration\", \"How do I configure the application?\"),\n",
    "        (\"Usage\", \"How do I run the main application?\"),\n",
    "        (\"Troubleshooting\", \"What should I do if I encounter errors?\"),\n",
    "        (\"Architecture\", \"How is the project structured?\"),\n",
    "        (\"Data\", \"What data does this project use and how is it processed?\"),\n",
    "    ]\n",
    "    \n",
    "    for category, question in test_scenarios:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Category: {category}\")\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        result = await workout_agent.run(user_prompt=question)\n",
    "        print(f\"\\nAnswer: {result.output}\")\n",
    "        \n",
    "        # Check if tool was used\n",
    "        tool_used = any(\n",
    "            hasattr(msg, 'tool_calls') and msg.tool_calls \n",
    "            for msg in result.new_messages()\n",
    "        )\n",
    "        print(f\"\\n‚úÖ Tool used: {tool_used}\")\n",
    "\n",
    "await comprehensive_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8c4399b4-e279-451b-a64b-3b1a83f57f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STEP 1: TEST SIMPLE LOGGING ===\n",
      "üß™ Testing with: How do I install this project?\n",
      "üîç Searching for: 'installation'\n",
      "üìä Found 5 results\n",
      "ü§ñ Agent response: Based on the documentation, here are the installation steps:\n",
      "\n",
      "**Prerequisites:**\n",
      "\n",
      "*   Python 3.9+\n",
      "* ...\n",
      "‚úÖ Saved simple log to: test_log_20251216_224214.json\n",
      "üìÅ File size: 825 bytes\n"
     ]
    }
   ],
   "source": [
    "# 5 evaluation\n",
    "import json\n",
    "import secrets\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "LOG_DIR = Path('logs')\n",
    "LOG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "async def test_simple_logging():\n",
    "    \"\"\"Just test logging with one question\"\"\"\n",
    "    \n",
    "    # Ask ONE simple question\n",
    "    question = \"How do I install this project?\"\n",
    "    print(f\"üß™ Testing with: {question}\")\n",
    "    \n",
    "    # Run agent\n",
    "    result = await workout_agent.run(user_prompt=question)\n",
    "    print(f\"ü§ñ Agent response: {result.output[:100]}...\")\n",
    "    \n",
    "    # Save to log file\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"test_log_{timestamp}.json\"\n",
    "    filepath = LOG_DIR / filename\n",
    "    \n",
    "    # Simple log (just save the basics)\n",
    "    simple_log = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"question\": question,\n",
    "        \"answer\": result.output,\n",
    "        \"agent_name\": \"workout_agent_test\"\n",
    "    }\n",
    "    \n",
    "    with open(filepath, \"w\") as f:\n",
    "        json.dump(simple_log, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ Saved simple log to: {filename}\")\n",
    "    \n",
    "    # Check file exists\n",
    "    if filepath.exists():\n",
    "        print(f\"üìÅ File size: {filepath.stat().st_size} bytes\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"‚ùå File not created!\")\n",
    "        return False\n",
    "\n",
    "# Run the test\n",
    "print(\"=== STEP 1: TEST SIMPLE LOGGING ===\")\n",
    "logging_works = await test_simple_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4e0ee4e8-a218-47b3-a3b0-7b567895921f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CHECKING LOGS DIRECTORY ===\n",
      "Found 2 log files:\n",
      "  - test_log_20251216_224120.json\n",
      "  - test_log_20251216_224214.json\n"
     ]
    }
   ],
   "source": [
    "# Check logs directory\n",
    "print(\"\\n=== CHECKING LOGS DIRECTORY ===\")\n",
    "if LOG_DIR.exists():\n",
    "    log_files = list(LOG_DIR.glob(\"*.json\"))\n",
    "    print(f\"Found {len(log_files)} log files:\")\n",
    "    for file in log_files:\n",
    "        print(f\"  - {file.name}\")\n",
    "else:\n",
    "    print(\"‚ùå Logs directory doesn't exist!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "15de276c-5897-40f0-a51f-ea2117ba0ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Evaluation agent created with same OpenRouter setup\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "# Reuse the SAME model setup\n",
    "eval_openrouter_model = OpenAIChatModel(\n",
    "    model_name=\"google/gemma-3-27b-it:free\",  # Same model\n",
    "    provider=provider  # Same provider\n",
    ")\n",
    "\n",
    "# Simple evaluation schema\n",
    "class SimpleEvaluation(BaseModel):\n",
    "    is_relevant: bool\n",
    "    used_tools: bool\n",
    "    score: int  # 1-5\n",
    "\n",
    "# Simple evaluation agent\n",
    "simple_eval_agent = Agent(\n",
    "    name='simple_evaluator',\n",
    "    model=eval_openrouter_model,  # Use the same model setup\n",
    "    instructions=\"\"\"Judge if the answer is relevant to the question about a workout recommendation repository.\n",
    "    \n",
    "    IMPORTANT: The agent has access to a 'search_workout_docs' tool. \n",
    "    If the answer contains specific details from documentation (like \"Based on the documentation\", \n",
    "    \"According to README.md\", section references, file names, or specific technical details), \n",
    "    it likely used the search tool.\n",
    "    \n",
    "    Return true/false for relevance and tool usage, and a score from 1-5.\"\"\",\n",
    "    output_type=SimpleEvaluation\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Evaluation agent created with same OpenRouter setup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4acd3d12-d6a7-48d2-986a-d684fdd75684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TESTING BOTH AGENTS ===\n",
      "\n",
      "1. Testing workout_agent...\n",
      "üîç Searching for: 'project overview'\n",
      "üìä Found 5 results\n",
      "‚úÖ Main agent works: This project is a workout recommendation system. I...\n",
      "\n",
      "2. Testing evaluation agent...\n",
      "‚úÖ Evaluation agent works:\n",
      "   Relevant: True\n",
      "   Used tools: False\n",
      "   Score: 5/5\n"
     ]
    }
   ],
   "source": [
    "async def test_both_agents():\n",
    "    \"\"\"Test both agents work with OpenRouter\"\"\"\n",
    "    \n",
    "    print(\"=== TESTING BOTH AGENTS ===\")\n",
    "    \n",
    "    # Test 1: Main agent\n",
    "    print(\"\\n1. Testing workout_agent...\")\n",
    "    try:\n",
    "        result = await workout_agent.run(user_prompt=\"What is this project?\")\n",
    "        print(f\"‚úÖ Main agent works: {result.output[:50]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Main agent error: {e}\")\n",
    "    \n",
    "    # Test 2: Evaluation agent (with hardcoded example)\n",
    "    print(\"\\n2. Testing evaluation agent...\")\n",
    "    try:\n",
    "        test_prompt = \"\"\"\n",
    "        Question: How do I install this project?\n",
    "        \n",
    "        Answer: To install, clone the repo and run pip install.\n",
    "        \n",
    "        Evaluate relevance, tool usage, and score.\n",
    "        \"\"\"\n",
    "        \n",
    "        eval_result = await simple_eval_agent.run(test_prompt)\n",
    "        print(f\"‚úÖ Evaluation agent works:\")\n",
    "        print(f\"   Relevant: {eval_result.output.is_relevant}\")\n",
    "        print(f\"   Used tools: {eval_result.output.used_tools}\")\n",
    "        print(f\"   Score: {eval_result.output.score}/5\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Evaluation agent error: {e}\")\n",
    "\n",
    "# Run test\n",
    "await test_both_agents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a49cce58-fb98-4569-a24d-70ef60777275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP 4 IMPROVED: TEST WITH ACTUAL TOOL CHECKING ===\n",
      "\n",
      "=== TESTING WITH 3 QUESTIONS (IMPROVED) ===\n",
      "\n",
      "[1/3] Q: How do I install this project?\n",
      "üîç Searching for: 'installation'\n",
      "üìä Found 5 results\n",
      "   Response preview: Based on the documentation, here are the installation steps:\n",
      "\n",
      "**Prerequisites:**...\n",
      "   üîß ACTUAL tool calls: 1\n",
      "   ‚úÖ Relevant: True\n",
      "   ‚öôÔ∏è  Used tools (ACTUAL): True\n",
      "   ‚≠ê Score: 5/5\n",
      "\n",
      "[2/3] Q: What dataset does this use?\n",
      "üîç Searching for: 'dataset source'\n",
      "üìä Found 5 results\n",
      "   Response preview: This project uses the \"Gym Members Exercise Dataset\" from Kaggle ([https://www.k...\n",
      "   üîß ACTUAL tool calls: 1\n",
      "   ‚úÖ Relevant: True\n",
      "   ‚öôÔ∏è  Used tools (ACTUAL): True\n",
      "   ‚≠ê Score: 5/5\n",
      "\n",
      "[3/3] Q: How do I run the API?\n",
      "üîç Searching for: 'run API'\n",
      "üìä Found 5 results\n",
      "   Response preview: Based on the documentation, to run the API:\n",
      "\n",
      "1.  **Base URL:** The API can be ac...\n",
      "   üîß ACTUAL tool calls: 1\n",
      "   ‚úÖ Relevant: True\n",
      "   ‚öôÔ∏è  Used tools (ACTUAL): True\n",
      "   ‚≠ê Score: 5/5\n"
     ]
    }
   ],
   "source": [
    "# Just 3 test questions to start\n",
    "BASIC_TEST_QUESTIONS = [\n",
    "    \"How do I install this project?\",\n",
    "    \"What dataset does this use?\",\n",
    "    \"How do I run the API?\"\n",
    "]\n",
    "\n",
    "async def test_with_3_questions_improved():\n",
    "    \"\"\"Test with actual tool usage checking\"\"\"\n",
    "    \n",
    "    print(f\"\\n=== TESTING WITH 3 QUESTIONS (IMPROVED) ===\")\n",
    "    \n",
    "    for i, question in enumerate(BASIC_TEST_QUESTIONS, 1):\n",
    "        print(f\"\\n[{i}/3] Q: {question}\")\n",
    "        \n",
    "        # Run agent\n",
    "        result = await workout_agent.run(user_prompt=question)\n",
    "        print(f\"   Response preview: {result.output[:80]}...\")\n",
    "        \n",
    "        # ACTUALLY CHECK if tools were used\n",
    "        tool_used = False\n",
    "        for msg in result.new_messages():\n",
    "            if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
    "                tool_used = True\n",
    "                print(f\"   üîß ACTUAL tool calls: {len(msg.tool_calls)}\")\n",
    "        \n",
    "        # Simple evaluation (just for relevance and score)\n",
    "        eval_prompt = f\"Question: {question}\\nAnswer: {result.output[:300]}...\"\n",
    "        eval_result = await simple_eval_agent.run(eval_prompt)\n",
    "        \n",
    "        print(f\"   ‚úÖ Relevant: {eval_result.output.is_relevant}\")\n",
    "        print(f\"   ‚öôÔ∏è  Used tools (ACTUAL): {tool_used}\")  # Real check!\n",
    "        print(f\"   ‚≠ê Score: {eval_result.output.score}/5\")\n",
    "\n",
    "# Test the improved version\n",
    "print(\"\\n=== STEP 4 IMPROVED: TEST WITH ACTUAL TOOL CHECKING ===\")\n",
    "await test_with_3_questions_improved()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "75efe3c6-024f-4682-90c8-bd314db613c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RUNNING IMPROVED WORKFLOW ===\n",
      "\n",
      "==================================================\n",
      "IMPROVED EVALUATION WORKFLOW\n",
      "==================================================\n",
      "\n",
      "[1/3] Processing: How do I install this project?\n",
      "üîç Searching for: 'installation'\n",
      "üìä Found 5 results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"D:\\datatalksclub\\AI Agent email course\\aihero\\course\\.venv\\Lib\\site-packages\\pydantic_ai\\models\\openai.py\", line 556, in _completions_create\n",
      "    return await self.client.chat.completions.create(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\datatalksclub\\AI Agent email course\\aihero\\course\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2678, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\datatalksclub\\AI Agent email course\\aihero\\course\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1794, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\datatalksclub\\AI Agent email course\\aihero\\course\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1594, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '50', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1765929600000'}, 'provider_name': None}}, 'user_id': 'user_316HK2MkISAE2wThFSExQiZYdQ7'}\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\datatalksclub\\AI Agent email course\\aihero\\course\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3699, in run_code\n",
      "    await eval(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24288\\2143838285.py\", line 77, in <module>\n",
      "    improved_results = await simple_evaluation_workflow_improved()\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24288\\2143838285.py\", line 14, in simple_evaluation_workflow_improved\n",
      "    result = await workout_agent.run(user_prompt=question)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\datatalksclub\\AI Agent email course\\aihero\\course\\.venv\\Lib\\site-packages\\pydantic_ai\\agent\\abstract.py\", line 251, in run\n",
      "    async with self.iter(\n",
      "  File \"C:\\Program Files\\Python312\\Lib\\contextlib.py\", line 231, in __aexit__\n",
      "    await self.gen.athrow(value)\n",
      "  File \"D:\\datatalksclub\\AI Agent email course\\aihero\\course\\.venv\\Lib\\site-packages\\pydantic_ai\\agent\\__init__.py\", line 673, in iter\n",
      "    async with graph.iter(\n",
      "  File \"C:\\Program Files\\Python312\\Lib\\contextlib.py\", line 231, in __aexit__\n",
      "    await self.gen.athrow(value)\n",
      "  File \"D:\\datatalksclub\\AI Agent email course\\aihero\\course\\.venv\\Lib\\site-packages\\pydantic_graph\\beta\\graph.py\", line 270, in iter\n",
      "    async with GraphRun[StateT, DepsT, OutputT](\n",
      "  File \"D:\\datatalksclub\\AI Agent email course\\aihero\\course\\.venv\\Lib\\site-packages\\pydantic_graph\\beta\\graph.py\", line 423, in __aexit__\n",
      "    await self._async_exit_stack.__aexit__(exc_type, exc_val, exc_tb)\n",
      "  File \"C:\\Program Files\\Python312\\Lib\\contextlib.py\", line 754, in __aexit__\n",
      "    raise exc_details[1]\n",
      "  File \"C:\\Program Files\\Python312\\Lib\\contextlib.py\", line 735, in __aexit__\n",
      "    cb_suppress = cb(*exc_details)\n",
      "                  ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Program Files\\Python312\\Lib\\contextlib.py\", line 158, in __exit__\n",
      "    self.gen.throw(value)\n",
      "  File \"D:\\datatalksclub\\AI Agent email course\\aihero\\course\\.venv\\Lib\\site-packages\\pydantic_graph\\beta\\graph.py\", line 981, in _unwrap_exception_groups\n",
      "    raise exception\n",
      "  File \"D:\\datatalksclub\\AI Agent email course\\aihero\\course\\.venv\\Lib\\site-packages\\pydantic_graph\\beta\\graph.py\", line 750, in _run_tracked_task\n",
      "    result = await self._run_task(t_)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\datatalksclub\\AI Agent email course\\aihero\\course\\.venv\\Lib\\site-packages\\pydantic_graph\\beta\\graph.py\", line 782, in _run_task\n",
      "    output = await node.call(step_context)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\datatalksclub\\AI Agent email course\\aihero\\course\\.venv\\Lib\\site-packages\\pydantic_graph\\beta\\step.py\", line 253, in _call_node\n",
      "    return await node.run(GraphRunContext(state=ctx.state, deps=ctx.deps))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\datatalksclub\\AI Agent email course\\aihero\\course\\.venv\\Lib\\site-packages\\pydantic_ai\\_agent_graph.py\", line 440, in run\n",
      "    return await self._make_request(ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\datatalksclub\\AI Agent email course\\aihero\\course\\.venv\\Lib\\site-packages\\pydantic_ai\\_agent_graph.py\", line 482, in _make_request\n",
      "    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\datatalksclub\\AI Agent email course\\aihero\\course\\.venv\\Lib\\site-packages\\pydantic_ai\\models\\openai.py\", line 474, in request\n",
      "    response = await self._completions_create(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\datatalksclub\\AI Agent email course\\aihero\\course\\.venv\\Lib\\site-packages\\pydantic_ai\\models\\openai.py\", line 588, in _completions_create\n",
      "    raise ModelHTTPError(status_code=status_code, model_name=self.model_name, body=e.body) from e\n",
      "pydantic_ai.exceptions.ModelHTTPError: status_code: 429, model_name: google/gemma-3-27b-it:free, body: {'message': 'Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '50', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1765929600000'}, 'provider_name': None}}\n"
     ]
    }
   ],
   "source": [
    "async def simple_evaluation_workflow_improved():\n",
    "    \"\"\"Evaluation workflow that actually checks tool usage\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"IMPROVED EVALUATION WORKFLOW\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    evaluations = []\n",
    "    \n",
    "    for i, question in enumerate(BASIC_TEST_QUESTIONS, 1):\n",
    "        print(f\"\\n[{i}/{len(BASIC_TEST_QUESTIONS)}] Processing: {question}\")\n",
    "        \n",
    "        # Run agent and get FULL result (with messages)\n",
    "        result = await workout_agent.run(user_prompt=question)\n",
    "        \n",
    "        # ACTUAL tool usage check\n",
    "        actual_tool_used = False\n",
    "        tool_call_details = []\n",
    "        \n",
    "        for msg in result.new_messages():\n",
    "            if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
    "                actual_tool_used = True\n",
    "                for tool_call in msg.tool_calls:\n",
    "                    tool_name = getattr(tool_call, 'tool_name', 'unknown')\n",
    "                    tool_call_details.append(f\"{tool_name}({tool_call.args})\")\n",
    "        \n",
    "        # Evaluation for relevance and quality\n",
    "        eval_prompt = f\"\"\"\n",
    "        Question: {question}\n",
    "        Answer: {result.output[:400]}...\n",
    "        \n",
    "        Evaluate if this answer is relevant and helpful for someone asking about the workout repository.\n",
    "        Score from 1-5.\n",
    "        \"\"\"\n",
    "        \n",
    "        eval_result = await simple_eval_agent.run(eval_prompt)\n",
    "        \n",
    "        evaluation = {\n",
    "            \"question\": question,\n",
    "            \"score\": eval_result.output.score,\n",
    "            \"relevant\": eval_result.output.is_relevant,\n",
    "            \"used_tools_actual\": actual_tool_used,  # REAL check\n",
    "            \"used_tools_guess\": eval_result.output.used_tools,  # AI's guess\n",
    "            \"tool_calls\": tool_call_details\n",
    "        }\n",
    "        evaluations.append(evaluation)\n",
    "        \n",
    "        print(f\"   ‚úÖ Relevant: {evaluation['relevant']}\")\n",
    "        print(f\"   üîß Tools used (ACTUAL): {evaluation['used_tools_actual']}\")\n",
    "        if evaluation['used_tools_actual']:\n",
    "            print(f\"   üìù Tool calls: {', '.join(evaluation['tool_calls'])}\")\n",
    "        print(f\"   ‚≠ê Score: {evaluation['score']}/5\")\n",
    "    \n",
    "    # Show results\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"FINAL RESULTS (WITH ACTUAL TOOL CHECKING)\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    avg_score = sum(e['score'] for e in evaluations) / len(evaluations)\n",
    "    actual_tool_rate = sum(1 for e in evaluations if e['used_tools_actual']) / len(evaluations) * 100\n",
    "    guessed_tool_rate = sum(1 for e in evaluations if e['used_tools_guess']) / len(evaluations) * 100\n",
    "    \n",
    "    print(f\"\\nüìä Metrics:\")\n",
    "    print(f\"  Average Score: {avg_score:.2f}/5\")\n",
    "    print(f\"  Actual Tool Usage Rate: {actual_tool_rate:.1f}%\")\n",
    "    print(f\"  AI-Guessed Tool Usage: {guessed_tool_rate:.1f}%\")\n",
    "    \n",
    "    print(f\"\\nüîç Tool Usage Accuracy:\")\n",
    "    correct_guesses = sum(1 for e in evaluations if e['used_tools_actual'] == e['used_tools_guess'])\n",
    "    accuracy = correct_guesses / len(evaluations) * 100\n",
    "    print(f\"  AI correctly guessed tool usage: {accuracy:.1f}% of the time\")\n",
    "    \n",
    "    return evaluations\n",
    "\n",
    "# Run improved workflow\n",
    "print(\"\\n=== RUNNING IMPROVED WORKFLOW ===\")\n",
    "improved_results = await simple_evaluation_workflow_improved()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5c4723c6-9b8c-4e78-814b-ebbebd8583ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created: app\n",
      "‚úÖ Created: app/pyproject.toml\n",
      "\n",
      "üìã Files to create in app/ folder:\n",
      "  - ingest.py\n",
      "  - search_tools.py\n",
      "  - search_agent.py\n",
      "  - logs.py\n",
      "  - app.py\n",
      "  - requirements.txt\n",
      "  - README.md\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Create app folder (as per docs)\n",
    "app_folder = Path(\"app\")\n",
    "app_folder.mkdir(exist_ok=True)\n",
    "print(f\"‚úÖ Created: {app_folder}\")\n",
    "\n",
    "# Create pyproject.toml as per docs\n",
    "pyproject_content = '''[project]\n",
    "name = \"doc-agent-app\"\n",
    "version = \"0.1.0\"\n",
    "description = \"Documentation Assistant Agent\"\n",
    "authors = [\n",
    "    {name = \"You\", email = \"you@example.com\"}\n",
    "]\n",
    "dependencies = [\n",
    "    \"minsearch>=0.0.5\",\n",
    "    \"openai>=1.108.2\",\n",
    "    \"pydantic-ai==1.0.9\",\n",
    "    \"python-frontmatter>=1.1.0\",\n",
    "    \"requests>=2.32.5\",\n",
    "    \"streamlit>=1.35.0\",\n",
    "]\n",
    "\n",
    "[build-system]\n",
    "requires = [\"hatchling\"]\n",
    "build-backend = \"hatchling.build\"\n",
    "'''\n",
    "\n",
    "# Write pyproject.toml\n",
    "(app_folder / \"pyproject.toml\").write_text(pyproject_content)\n",
    "print(f\"‚úÖ Created: app/pyproject.toml\")\n",
    "\n",
    "# Create other files as per docs\n",
    "files_to_create = [\n",
    "    \"ingest.py\",\n",
    "    \"search_tools.py\",\n",
    "    \"search_agent.py\",\n",
    "    \"logs.py\",\n",
    "    \"app.py\",\n",
    "    \"requirements.txt\",\n",
    "    \"README.md\"\n",
    "]\n",
    "\n",
    "print(f\"\\nüìã Files to create in app/ folder:\")\n",
    "for file in files_to_create:\n",
    "    print(f\"  - {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92ccb9c-9c4c-4e4c-a011-c73e6b5751a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
